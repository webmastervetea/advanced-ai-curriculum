## 游꺕 Embeddings Hiperb칩licos: Modelando Jerarqu칤as con Curvatura Negativa

Los m칠todos de *embedding* tradicionales (utilizados en *Word2Vec* o *Graph Embeddings*) mapean los datos a un espacio **euclidiano** (plano). El problema es que para representar estructuras jer치rquicas o complejas de grafos, estos espacios son inherentemente ineficientes, obligando a usar una dimensi칩n innecesariamente alta para compensar la falta de curvatura.

La **Geometr칤a Hiperb칩lica** ofrece una soluci칩n natural, ya que su curvatura negativa permite una expansi칩n exponencial del espacio, ideal para modelar la naturaleza exponencial de los 치rboles y las jerarqu칤as.

---

### 1. La Ineficiencia del Espacio Euclidiano

Considere un **치rbol perfecto de ramificaci칩n $b$ y profundidad $d$**. El n칰mero de nodos crece exponencialmente con la profundidad, siguiendo $O(b^d)$.

* **Espacio Euclidiano:** La distancia entre dos puntos solo puede crecer linealmente con las coordenadas. Para encajar exponencialmente m치s nodos a medida que la distancia del centro aumenta, el espacio euclidiano requiere que la **dimensi칩n** sea prohibitivamente grande, o que los nodos se api침en densamente, lo que hace que los *embeddings* sean ineficaces y pierdan la jerarqu칤a.

* **Ejemplo:** En un espacio 2D, para que los nodos de un 치rbol sean distintos, tendr칤an que estar muy lejos del centro, ocupando mucho espacio, o estar muy cerca unos de otros.

### 2. La Ventaja de la Geometr칤a Hiperb칩lica

La geometr칤a hiperb칩lica (generalmente modelada por el **Disco de Poincar칠** o el **Modelo de Hiperboloide**) exhibe **curvatura negativa** constante.

#### A. Expansi칩n Exponencial
En un espacio hiperb칩lico, la circunferencia de un c칤rculo (y por lo tanto el volumen del espacio) crece **exponencialmente** con el radio.

* **Propiedad Clave:**
    * **Euclidiano:** Circunferencia $\approx 2\pi r$ (crecimiento lineal)
    * **Hiperb칩lico:** 츼rea $\approx C \cdot e^{r}$ (crecimiento exponencial)
* **Relevancia:** Esta propiedad coincide perfectamente con el crecimiento exponencial de los nodos en una estructura jer치rquica. La geometr칤a hiperb칩lica permite encajar el vasto n칰mero de nodos de un 치rbol en un radio peque침o de **baja dimensi칩n** y, al mismo tiempo, preservar las distancias m칠tricas de la jerarqu칤a. 

#### B. Preservaci칩n de la Jerarqu칤a
En un *embedding* hiperb칩lico, los nodos se incrustan de la siguiente manera:

* **Nodos Ra칤z:** Se incrustan cerca del **centro** del espacio.
* **Hojas (Nodos Terminales):** Se incrustan cerca del **borde** (el l칤mite del disco).
* **Distancia:** La **distancia hiperb칩lica** entre dos nodos es una representaci칩n directa de su distancia jer치rquica o su proximidad en el grafo original.

### 3. Implementaci칩n de Embeddings Hiperb칩licos

El desaf칤o pr치ctico es que la optimizaci칩n (Descenso de Gradiente) es m치s compleja en espacios curvos que en espacios planos.

#### A. Modelo del Disco de Poincar칠 ($\mathbb{D}^n$)
Este es el modelo m치s popular. Es una *variedad* con curvatura negativa constante.

* **M칠trica:** La **m칠trica de Riemann** define la distancia $d(\mathbf{u}, \mathbf{v})$ entre dos puntos $\mathbf{u}$ y $\mathbf{v}$ en el disco. Esta f칩rmula es m치s compleja que la distancia euclidiana, lo que es la clave de la expansi칩n exponencial.
* **Optimizaci칩n (Mapeo):** Los optimizadores de *Deep Learning* (ej., Adam) operan en el espacio euclidiano. Para usarlos, los gradientes calculados en el espacio euclidiano deben proyectarse a la variedad curva mediante un **operador de mapeo (o transporte)** que tiene en cuenta la curvatura.
    * **Pasos:**
        1.  Calcular el gradiente euclidiano ($\nabla_{Euc}$) de la funci칩n de p칠rdida.
        2.  Mapear el gradiente al espacio tangente de la variedad.
        3.  Mover el punto actual $\mathbf{u}$ a la nueva posici칩n $\mathbf{v}$ a lo largo de la **geod칠sica** (el camino m치s corto en la variedad curva).

#### B. Aplicaci칩n en Redes Neuronales Gr치ficas (HGNNs)
Las GNNs hiperb칩licas (HGNNs) combinan la GNN tradicional (*message passing*) con la geometr칤a hiperb칩lica.

* **Funci칩n:** Cada capa de la HGNN no solo agrega la informaci칩n del vecino, sino que tambi칠n realiza el c치lculo de agregaci칩n **utilizando las operaciones geom칠tricas hiperb칩licas** (suma, promedio) para asegurar que el resultado se mantenga dentro de la variedad curva y que se preserve la estructura jer치rquica.
* **Beneficio:** Permite que las GNNs capturen la estructura jer치rquica inherente en muchos grafos (redes sociales, taxonom칤as de conocimiento) de manera m치s eficiente que sus contrapartes euclidianas.

### 4. Aplicaciones Clave

| Dominio | Estructura Intr칤nseca | Ventaja Hiperb칩lica |
| :--- | :--- | :--- |
| **Gr치ficos de Conocimiento** | Taxonom칤as (esquemas de clasificaci칩n, herencia de clases). | Preserva la relaci칩n "es-un-tipo-de" (jerarqu칤a). |
| **Redes Sociales/Web** | Grafos con topolog칤a de 치rbol, comunidades peque침as y nodos centrales. | La incrustaci칩n de baja dimensi칩n captura la naturaleza de "mundo peque침o". |
| **PLN (Representaciones Jer치rquicas)** | Jerarqu칤a de conceptos o relaciones l칠xicas (*WordNet*). | *Embeddings* de palabras que reflejan distancias taxon칩micas. |

---

### Conclusi칩n

La adopci칩n de los **Espacios Hiperb칩licos** en el *Deep Learning* representa un cambio de paradigma: reconocer que la geometr칤a del espacio de *embedding* debe coincidir con la geometr칤a intr칤nseca de los datos. Al utilizar la curvatura negativa para proporcionar la capacidad de expansi칩n exponencial necesaria, los *embeddings* hiperb칩licos permiten una representaci칩n **m치s fiel, compacta y de baja dimensi칩n** de estructuras jer치rquicas y de grafos complejos, superando las limitaciones de los m칠todos euclidianos.

---

Continua: [[37.1](https://github.com/webmastervetea/advanced-ai-curriculum/blob/main/temarios/37-1.md)] 
