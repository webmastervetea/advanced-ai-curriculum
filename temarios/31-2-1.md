

## ✨ Feature Engineering Automatizado: El Arte de la Transformación de Datos

La **Ingeniería de Características (*Feature Engineering*)** es el proceso de transformar los datos brutos en características que mejor representan el problema subyacente al modelo de *Machine Learning*. Cuando este proceso es automatizado, permite descubrir relaciones complejas que a menudo escapan a la intuición humana, impulsando significativamente el rendimiento del modelo.

---

### 1. El Problema: Datos Estructurados y Relacionales

Mientras que en dominios como la visión (CNNs) y el lenguaje (Transformers), la extracción de características es inherente a la arquitectura, en los **datos estructurados o relacionales** (bases de datos tabulares, *datasets* de clientes), la creación de características (*feature engineering*) sigue siendo la parte más crítica y manual del *pipeline* de ML.

El *Automated Feature Engineering (AFE)* busca generar características nuevas y significativas como **combinaciones, agregaciones y transformaciones** de las características existentes.

---

### 2. Métodos Basados en Algoritmos: Búsqueda y Combinación

Estos métodos utilizan algoritmos de búsqueda explícitos para combinar características de una manera sistemática.

#### A. Búsqueda con Algoritmos Genéticos (AFE-AG)

* **Principio:** Aplicar la metodología de Algoritmos Genéticos (AG) para encontrar combinaciones óptimas de operadores.
* **Mecanismo:**
    1.  **Población:** Cada individuo es una **fórmula de característica** (ej., `log(Edad) * sin(Ingresos)`).
    2.  **Mutación y Cruce (*Crossover*):** Los "cromosomas" (las fórmulas) se recombinan y se mutan introduciendo nuevos operadores o cambiando los existentes.
    3.  **Fitness:** El *fitness* se mide entrenando un modelo simple de ML (p. ej., un árbol de decisión) con la nueva característica y evaluando el rendimiento.
* **Ventaja:** Puede explorar un espacio de características vasto y encontrar transformaciones altamente no lineales.

#### B. Generación de Características Basada en Gramáticas

* **Principio:** Limitar el espacio de búsqueda utilizando una gramática formal que define las combinaciones de características válidas y lógicas.
* **Mecanismo:** La gramática define las operaciones permitidas (ej., `SUM(A, B)`, `MEAN(C) sobre D=d`) y las restricciones (ej., no dividir por cero). La búsqueda (RL, AG o búsqueda voraz) opera dentro de estas reglas lógicas.
* **Ventaja:** Asegura que las características generadas sean sintácticamente correctas y más interpretables.

#### C. *Featuretools* (Agregación y Profundidad)

Un *framework* popular que utiliza la **Profundidad de Características (*Feature Deepness*)** para generar características en *datasets* relacionales (múltiples tablas).

* **Concepto:** Utiliza **Primitivas de Características** (operadores de alto nivel como `COUNT`, `MAX`, `SUM`, `MODE`).
* **Mecanismo:** Genera características aplicando primitivas a través de las relaciones (joins) entre las tablas. Por ejemplo:
    $$\text{Característica Nueva} = \text{MEAN}(\text{Monto de Préstamo}) \text{ en todas las Transacciones por Cliente}$$
* **Ventaja:** Automáticamente maneja la complejidad de los datos relacionales sin requerir *expertise* en SQL o *joins* complejos.

---

### 3. Métodos Basados en Aprendizaje Profundo (*Deep Learning*)

Estos métodos son end-to-end, donde la arquitectura aprende una representación óptima de las características sin ninguna búsqueda explícita.

#### A. Autoencoders Apilados (Stacked Autoencoders)

* **Principio:** Utilizar una red para comprimir y luego reconstruir los datos de entrada, forzando a la capa de cuello de botella (*bottleneck*) a aprender una representación latente eficiente (nuevas características).
* **Mecanismo:**
    1.  El **Codificador** toma las características brutos ($\mathbf{x}$) y las mapea al espacio de características latentes ($\mathbf{z}$), donde $\mathbf{z}$ es la característica recién construida.
    2.  El **Decodificador** intenta reconstruir $\mathbf{x}$ a partir de $\mathbf{z}$.
    3.  Una vez entrenado, el codificador es aislado y su salida ($\mathbf{z}$) se utiliza como el nuevo conjunto de características de entrada para la tarea final (clasificación/regresión).
* **Ventaja:** Descubre características no lineales que son óptimas para la reconstrucción de los datos.

#### B. *Deep Forest* (Estrategias no Neuronales)

Aunque no es estrictamente *Deep Learning*, **Deep Forest** utiliza un enfoque de **aprendizaje en cascada** que se asemeja a la jerarquía de las redes neuronales para la extracción de características.

* **Mecanismo:** Las características de entrada pasan a través de múltiples niveles de **Bosques Aleatorios (*Random Forests*)**. La salida de la predicción de cada bosque se concatena con las características originales y se alimenta al siguiente nivel.
* **Ventaja:** La salida de cada nivel actúa como una característica construida y de alto nivel. Supera la necesidad de *backpropagation* y funciona bien con datos tabulares.

---

### 4. Conclusión

El *Automated Feature Engineering* es la próxima frontera en la mejora del rendimiento del modelo. Mientras que los métodos basados en **Algoritmos Genéticos** y **Gramáticas** son poderosos para la generación explícita y relacional de características, los enfoques basados en **Autoencoders** integran la extracción de características directamente en la red neuronal, produciendo *embeddings* latentes que son las características más informativas para la tarea final. La adopción de estas técnicas elimina el tedioso y subjetivo proceso manual, permitiendo a los científicos de datos centrarse en la modelización y la auditoría.
---

Continua: [[31-3-1](https://github.com/webmastervetea/advanced-ai-curriculum/blob/main/temarios/31-3-1.md)] 
