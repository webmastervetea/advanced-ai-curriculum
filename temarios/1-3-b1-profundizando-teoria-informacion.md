## üìù Profundizando en la Teor√≠a de la Informaci√≥n

---

## üå™Ô∏è Entrop√≠a Condicional ($H(X|Y)$): La Incertidumbre Residual

Si la **Entrop√≠a** ($H(X)$) mide la incertidumbre de una variable $X$ sin conocimiento previo, la **Entrop√≠a Condicional** ($H(X|Y)$) mide la incertidumbre **restante** de $X$ despu√©s de que se ha observado otra variable $Y$.

En esencia, $H(X|Y)$ responde a la pregunta: "Si ya conozco el valor de $Y$, ¬øcu√°nta incertidumbre me queda sobre $X$?"

### Definici√≥n Matem√°tica

La entrop√≠a condicional se define como el promedio de la entrop√≠a de $X$ para cada valor posible de $Y$, ponderado por la probabilidad de ese valor de $Y$. Utiliza la probabilidad conjunta $P(x, y)$ y la probabilidad condicional $P(x|y)$:

$$H(X|Y) = \sum_{y \in \mathcal{Y}} P(y) H(X|Y=y) = - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} P(x, y) \log_2 (P(x|y))$$

### Relaci√≥n con la Comunicaci√≥n

En un canal de comunicaci√≥n donde $X$ es la se√±al transmitida e $Y$ es la se√±al recibida:

* $H(X)$ es la incertidumbre del mensaje original.
* $H(Y|X)$ es la **entrop√≠a del error o ruido**. Mide la incertidumbre en la se√±al recibida ($Y$) dada la se√±al transmitida ($X$). Si el canal es **perfecto** (sin ruido), $H(Y|X) = 0$.
* $H(X|Y)$ es la **incertidumbre residual** en el mensaje original ($X$) despu√©s de haber visto la salida recibida ($Y$). Esto representa la probabilidad de que el mensaje se haya interpretado mal.

El v√≠nculo con la Informaci√≥n Mutua es directo:
$$I(X; Y) = H(X) - H(X|Y)$$
Es decir, la informaci√≥n mutua es la **reducci√≥n** de la incertidumbre original ($H(X)$) lograda al conocer la otra variable ($Y$).

---

## üì° El Teorema del Canal Ruidoso de Shannon: El L√≠mite de la Transmisi√≥n

El **Teorema del Canal Ruidoso de Shannon** (o Segundo Teorema de Shannon) es el resultado m√°s fundamental y trascendental de la Teor√≠a de la Informaci√≥n. Responde a la pregunta: **¬øCu√°l es la tasa de datos m√°xima a la que la informaci√≥n puede transmitirse de forma fiable a trav√©s de un canal con ruido?**

### 1. La Capacidad del Canal ($C$)

Shannon defini√≥ la **Capacidad del Canal ($C$)** como la tasa m√°xima de Informaci√≥n Mutua posible entre la entrada ($X$) y la salida ($Y$) sobre todas las posibles distribuciones de entrada $P(X)$:

$$C = \max_{P(X)} I(X; Y)$$

La capacidad $C$ se mide en **bits por segundo (bps)** o **bits por uso del canal**. Es una propiedad intr√≠nseca del canal de comunicaci√≥n (como un cable o una conexi√≥n inal√°mbrica).

### 2. El Teorema

El teorema establece dos afirmaciones cruciales:

* **Si la Tasa de Informaci√≥n ($R$) es menor que la Capacidad ($C$):**
    $$R < C$$
    Es te√≥ricamente posible dise√±ar un sistema de codificaci√≥n que permita transmitir la informaci√≥n con una **tasa de error arbitrariamente peque√±a**. Esto significa que podemos enviar datos de manera fiable siempre que no excedamos el l√≠mite del canal.
* **Si la Tasa de Informaci√≥n ($R$) es mayor que la Capacidad ($C$):**
    $$R > C$$
    La tasa de error en la recepci√≥n **no puede** hacerse arbitrariamente peque√±a. No importa qu√© tan inteligente sea el sistema de codificaci√≥n que utilicemos, siempre habr√° una tasa de error m√≠nima en la recepci√≥n, ya que estamos intentando forzar demasiada informaci√≥n a trav√©s de un canal limitado.

### 3. La F√≥rmula de la Capacidad (F√≥rmula de Shannon-Hartley)

Para el caso espec√≠fico de un **canal Gaussiano de Ruido Blanco Aditivo (AWGN)**, la capacidad del canal $C$ puede calcularse con una f√≥rmula famosa:

$$C = B \log_2 \left( 1 + \frac{S}{N} \right)$$

Donde:
* $C$: Capacidad del canal (en bits por segundo).
* $B$: Ancho de banda del canal (en Hertz).
* $S/N$: La **Relaci√≥n Se√±al a Ruido (SNR)**, que es la relaci√≥n de la potencia promedio de la se√±al recibida ($S$) sobre la potencia promedio del ruido o interferencia ($N$).



Esta f√≥rmula tiene implicaciones profundas:

1.  **La Banda Ancha Importa ($B$):** Duplicar el ancho de banda duplica directamente la capacidad.
2.  **La Calidad de la Se√±al Importa ($S/N$):** Mejorar la relaci√≥n se√±al a ruido aumenta la capacidad logar√≠tmicamente.
3.  **El L√≠mite Absoluto:** La f√≥rmula establece un **l√≠mite superior te√≥rico** para la tasa de datos, sin importar la complejidad de la tecnolog√≠a utilizada. Las tecnolog√≠as modernas (como el 5G y el Wi-Fi 6) se esfuerzan por acercarse a este l√≠mite de Shannon.


---

Continua: [[2-1](https://github.com/webmastervetea/advanced-ai-curriculum/blob/main/temarios/2-1-redes-neuronales-recurrentes-memoria-maquinas.md)] 
