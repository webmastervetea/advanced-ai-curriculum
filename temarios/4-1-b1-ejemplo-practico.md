## 游냕 5. Ejemplo Pr치ctico: DQN y Flappy Bird

**Flappy Bird** es un entorno ideal para ilustrar el funcionamiento de DQN porque tiene un **espacio de estados de alta dimensi칩n** (debido a las im치genes de la pantalla), pero un **espacio de acciones discreto** muy simple (solo dos acciones posibles: **saltar** o **no saltar**).

### A. Definici칩n del Entorno para DQN

1.  **Estado ($s$):** En lugar de la imagen completa de la pantalla (lo que ralentizar칤a el entrenamiento), el estado se define generalmente como un vector de caracter칤sticas cruciales o una pila de los 칰ltimos 4 *frames* procesados. Las caracter칤sticas clave pueden incluir:
    * Posici칩n vertical del p치jaro.
    * Velocidad vertical del p치jaro.
    * Distancia horizontal al siguiente tubo.
    * Altura del hueco del siguiente tubo.
2.  **Acci칩n ($a$):**
    * $a=0$: No saltar (caer).
    * $a=1$: Saltar (*Flap*).
3.  **Recompensa ($r$):**
    * $r=+1$: El p치jaro pasa con 칠xito un tubo.
    * $r=+0.1$: El p치jaro sobrevive un paso de tiempo.
    * $r=-1$ o $-10$: El p치jaro choca contra un tubo o toca el suelo.

### B. La Red Neuronal (DQN)

La red DQN es una red neuronal que toma el **estado $s$** como entrada y produce **dos valores Q** como salida (uno para cada acci칩n: $Q(s, \text{saltar})$ y $Q(s, \text{no saltar})$).

* **Arquitectura:** Para las caracter칤sticas extra칤das, se utilizar칤an capas *fully connected*. Si se utiliza una pila de *frames* brutos (im치genes), la arquitectura ser칤a una **CNN** (Red Neuronal Convolucional) seguida de capas densas.
* **Decisi칩n de Acci칩n:** El agente sigue una pol칤tica $\epsilon$-greedy. Con probabilidad $\epsilon$, elige una acci칩n al azar (exploraci칩n). Con probabilidad $1-\epsilon$, elige la acci칩n que maximiza el valor Q: $a = \arg\max_a Q(s, a; \theta)$.

### C. Aprendizaje

El agente juega millones de partidas, almacenando cada transici칩n ($\langle s, a, r, s' \rangle$) en el **Buffer de Memoria de Experiencia**.

1.  **Muestreo:** Se muestrean mini-lotes aleatorios del *buffer*.
2.  **C치lculo del Objetivo:** Se calcula el valor objetivo $y$:
    $$y = r + \gamma \max_{a'} Q_{\text{target}}(s', a'; \theta^-)$$
3.  **Actualizaci칩n:** La red principal $Q(s, a; \theta)$ se actualiza para que su valor predicho se acerque a $y$.

De esta manera, el DQN aprende que, si la distancia al tubo es corta y la posici칩n es baja, la acci칩n de "saltar" tiene un valor $Q$ mucho m치s alto que la acci칩n de "no saltar".

---

## 丘뙖잺 6. Proximal Policy Optimization (PPO): El Est치ndar Moderno

Si bien A2C/A3C mejoraron el m칠todo de gradiente de pol칤tica, el algoritmo que se ha convertido en el est치ndar de facto en Deep RL (utilizado por OpenAI y DeepMind en la mayor칤a de sus trabajos recientes, incluyendo entrenamiento de rob칩tica y juegos complejos) es el **Proximal Policy Optimization (PPO)**.

PPO es un algoritmo **Actor-Critic** que resuelve un problema clave de los m칠todos Actor-Critic simples: su **sensibilidad al tama침o del paso de aprendizaje**.

### A. El Problema de las Pol칤ticas de Gradiente

Los algoritmos de Gradiente de Pol칤tica simples (como A2C) actualizan la pol칤tica tomando un paso del gradiente de recompensa. Si este paso es **demasiado grande**, la nueva pol칤tica puede ser significativamente peor que la anterior, lo que provoca que el entrenamiento se vuelva inestable o que el modelo "caiga" fuera de un buen m칤nimo.

### B. El Mecanismo de PPO: La P칠rdida Clipada (Clipped Loss)

PPO introduce una **funci칩n de p칠rdida modificada** que restringe la magnitud del cambio de pol칤tica en cada paso de optimizaci칩n. Esto garantiza que la nueva pol칤tica ($\pi_{\text{new}}$) se mantenga **pr칩xima** a la pol칤tica anterior ($\pi_{\text{old}}$), lo que resulta en un entrenamiento mucho m치s estable y robusto.

La funci칩n de p칠rdida de PPO, $\mathcal{L}^{\text{CLIP}}(\theta)$, es una funci칩n compleja que incluye el t칠rmino de la **Ventaja** ($A_t$) y un factor de ratio:

$$\mathcal{L}^{\text{CLIP}}(\theta) = \mathbb{E}_t \left[ \min(r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t) \right]$$

Donde:
* $r_t(\theta) = \frac{\pi_{\text{new}}(a_t|s_t)}{\pi_{\text{old}}(a_t|s_t)}$ es el **ratio de probabilidad** (qu칠 tan probable es la acci칩n en la pol칤tica nueva en comparaci칩n con la vieja).
* $\epsilon$ es el **hiperpar치metro de *clipping*** (t칤picamente 0.1 o 0.2), que define la regi칩n permitida de cambio.



### C. Funcionamiento del Clipping

La funci칩n $\min(\dots)$ asegura que la actualizaci칩n de la pol칤tica sea limitada:

1.  Si la nueva pol칤tica es **mucho mejor** que la anterior ($r_t(\theta)$ es muy grande y $A_t$ es positiva), la actualizaci칩n se **limita** a $(1+\epsilon)A_t$. Esto evita pasos excesivamente grandes.
2.  Si la nueva pol칤tica es **mucho peor** que la anterior ($r_t(\theta)$ es muy peque침a y $A_t$ es negativa), la actualizaci칩n se **limita** de manera similar, evitando que el modelo se castigue demasiado.

Al mantener el cambio de pol칤tica **dentro de un vecindario seguro** (el rango $[1-\epsilon, 1+\epsilon]$), PPO logra la estabilidad de los m칠todos de optimizaci칩n por lotes mientras mantiene la potencia de los m칠todos de gradiente de pol칤tica. Esto lo convierte en uno de los algoritmos m치s utilizados para el entrenamiento de IA en entornos complejos.
