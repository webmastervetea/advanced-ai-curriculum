# üï∏Ô∏è Deep Learning en Datos No Euclidianos: El Poder de las Redes Neuronales Gr√°ficas (GNNs)

Tradicionalmente, el *Deep Learning* (DL) ha demostrado un √©xito extraordinario en datos estructurados en espacios **euclidianos** regulares, como im√°genes (cuadr√≠culas 2D) o texto/audio (secuencias 1D). Sin embargo, muchos conjuntos de datos del mundo real tienen una **estructura no euclidiana**, donde los puntos de datos no residen en una cuadr√≠cula plana, sino en una **estructura de grafo** o **manifold**.

La **Redes Neuronales Gr√°ficas (*Graph Neural Networks*, GNNs)** son una clase de modelos DL dise√±ados espec√≠ficamente para manejar esta estructura compleja, abriendo un nuevo abanico de aplicaciones.

## 1. ¬øQu√© son los Datos No Euclidianos?

Los datos se consideran no euclidianos si su topolog√≠a se describe mejor mediante un **grafo ($G$)**, donde las relaciones entre los puntos de datos (nodos) son expl√≠citas y variables.

### A. Elementos de un Grafo

Un grafo $G$ se define por un conjunto de **nodos (v√©rtices, $V$)** y un conjunto de **aristas (bordes, $E$)** que conectan pares de nodos.

* **Nodos:** Representan entidades (ej. personas, mol√©culas, p√°ginas web).
* **Aristas:** Representan relaciones (ej. amistad, enlace qu√≠mico, hiperv√≠nculo).

A diferencia de una imagen (donde cada p√≠xel tiene exactamente 4 u 8 vecinos fijos), en un grafo, cada nodo tiene un n√∫mero **variable de vecinos**, lo que hace que los m√©todos tradicionales de DL (como las Convolucionales o Recurrentes) sean ineficaces.

### B. Ejemplos de Datos No Euclidianos

* **Redes Sociales:** Los usuarios son nodos, las amistades son aristas.
* **Qu√≠mica/Biolog√≠a:** Los √°tomos son nodos, los enlaces qu√≠micos son aristas (estructura molecular).
* **Sistemas de Transporte:** Las ciudades son nodos, las carreteras son aristas.
* **Gr√°ficos de Conocimiento (*Knowledge Graphs*):** Entidades y sus relaciones sem√°nticas.

---

## 2. El Paradigma de las Redes Neuronales Gr√°ficas (GNNs)

Las GNNs generalizan la operaci√≥n de convoluci√≥n (propia de las CNNs) a la estructura irregular de los grafos. Esto se logra mediante la **Propagaci√≥n de Mensajes (*Message Passing*)**.

### A. El Proceso de Propagaci√≥n de Mensajes

El objetivo de una GNN es generar una representaci√≥n vectorial (un *embedding*) para cada nodo, que capture tanto sus caracter√≠sticas locales como su posici√≥n estructural en el grafo. El proceso es iterativo a trav√©s de $L$ capas:

1.  **Generaci√≥n de Mensajes ($M_{uv}$):** En cada iteraci√≥n $k$, cada nodo $u$ genera un **mensaje** para su vecino $v$, basado en el *embedding* del nodo en la capa anterior ($h_u^{(k-1)}$) y las caracter√≠sticas de la arista.
2.  **Agregaci√≥n ($\alpha_v$):** Cada nodo $v$ **agrega** los mensajes recibidos de todos sus vecinos ($u \in \mathcal{N}(v)$). La funci√≥n de agregaci√≥n debe ser **invariante a la permutaci√≥n** (es decir, el orden en que se reciben los mensajes no debe afectar el resultado, por ejemplo, usando la suma o el promedio).
3.  **Actualizaci√≥n ($h_v^{(k)}$):** El *embedding* del nodo $v$ se actualiza combinando el mensaje agregado ($\alpha_v$) con el *embedding* anterior ($h_v^{(k-1)}$).

$$\mathbf{h}_v^{(k)} = \text{Actualizar} \left( \mathbf{h}_v^{(k-1)}, \text{Agregaci√≥n}_{u \in \mathcal{N}(v)} (\mathbf{h}_u^{(k-1)}) \right)$$

Despu√©s de $L$ capas, el *embedding* final $\mathbf{h}_v^{(L)}$ habr√° capturado la informaci√≥n estructural de su vecindario de $L$ saltos. 

### B. Variantes Populares de GNNs

* **GCN (Graph Convolutional Networks):** Utilizan una formulaci√≥n simple y eficiente basada en el espectro del grafo para agregar informaci√≥n.
* **GraphSAGE (Graph SAmple and aggreGatE):** Modelo inductivo que aprende una funci√≥n de agregaci√≥n generalizable a grafos no vistos. Es clave para grafos muy grandes, ya que solo muestrea una peque√±a porci√≥n de vecinos en lugar de procesar todo el vecindario.
* **GAT (Graph Attention Networks):** Utilizan mecanismos de **atenci√≥n** para asignar pesos variables a los mensajes entrantes, permitiendo que la GNN decida qu√© vecinos son m√°s importantes para el *embedding* del nodo actual.

---

## 3. Aplicaciones Clave de las GNNs

La capacidad de modelar expl√≠citamente las relaciones ha llevado a las GNNs a superar los m√©todos tradicionales en varios dominios:

### A. Redes Sociales y Recomendaci√≥n

* **Clasificaci√≥n de Nodos:** Predecir si un usuario (nodo) adoptar√° un producto o si un perfil es un *bot*.
* **Predicci√≥n de Enlaces:** Predecir si dos usuarios deber√≠an ser amigos o si un producto deber√≠a ser recomendado a un usuario. Las GNNs modelan c√≥mo las preferencias de los vecinos afectan las preferencias del usuario.

### B. Biolog√≠a y Descubrimiento de F√°rmacos

* **Predicci√≥n de Propiedades Moleculares:** Las mol√©culas son grafos (√°tomos son nodos, enlaces son aristas). Las GNNs se utilizan para predecir si una mol√©cula tendr√° cierta propiedad (ej. toxicidad o solubilidad) directamente a partir de su estructura de grafo.
* **Interacciones Prote√≠na-Prote√≠na:** Modelar la red de interacciones entre prote√≠nas para comprender mecanismos biol√≥gicos.

### C. Sistemas de Transporte y Log√≠stica

* **Predicci√≥n de Tr√°fico:** Modelar las intersecciones como nodos y las carreteras como aristas. Las GNNs pueden predecir la congesti√≥n modelando c√≥mo el flujo de tr√°fico en un segmento de carretera afecta a los segmentos adyacentes.

### D. Gr√°ficos de Conocimiento

* **Completado de Enlaces:** Inferir nuevas relaciones o hechos a partir de una base de conocimiento incompleta (ej. si A es padre de B, y B es padre de C, inferir la relaci√≥n de A con C).

## 4. Desaf√≠os en el Deep Learning de Grafos

* **Escalabilidad:** Las operaciones matriciales sobre la matriz de adyacencia del grafo pueden ser prohibitivamente lentas en grafos con miles de millones de nodos. GraphSAGE y el muestreo son soluciones activas.
* **Over-smoothing (Sobre-suavizado):** En GNNs profundas (muchas capas), las representaciones de todos los nodos tienden a converger, volvi√©ndose indistinguibles. Esto limita el n√∫mero de capas que se pueden apilar.
* **Heterogeneidad:** Los grafos reales a menudo tienen diferentes tipos de nodos y aristas (Grafos Heterog√©neos), lo que requiere arquitecturas m√°s complejas que modelen cada tipo de relaci√≥n por separado.

La Programaci√≥n Diferenciable, combinada con la estructura relacional de los grafos, posiciona a las GNNs como la tecnolog√≠a m√°s prometedora para dar sentido a los datos interconectados y relacionales del mundo.
