##  El Truco de la Reparametrizaci贸n en Detalle

El **Truco de la Reparametrizaci贸n** es la innovaci贸n t茅cnica que hizo posible entrenar los VAEs. Su necesidad surge del uso de la **Divergencia KL**, que penaliza la diferencia entre la distribuci贸n aprendida $q(\mathbf{z}|\mathbf{x})$ y la *a priori* $p(\mathbf{z})$.

### El Problema de la Muestra

En un VAE, el codificador produce los par谩metros ($\boldsymbol{\mu}$ y $\boldsymbol{\sigma}$) de la distribuci贸n latente. Luego, el vector latente $\mathbf{z}$ se obtiene mediante un **muestreo estoc谩stico** de esa distribuci贸n:

$$\mathbf{z} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\sigma}^2 \mathbf{I})$$

Durante el entrenamiento, el error de reconstrucci贸n depende de este $\mathbf{z}$. Para que el algoritmo de **retropropagaci贸n (*backpropagation*)** funcione, el gradiente debe fluir de la p茅rdida de vuelta a los pesos del codificador, es decir, a $\boldsymbol{\mu}$ y $\boldsymbol{\sigma}$. Sin embargo, el **proceso de muestreo es no diferenciable**, rompiendo la cadena de gradientes.

### La Soluci贸n: Separar la Aleatoriedad

El truco consiste en reformular el muestreo de una variable aleatoria $\mathbf{z}$ con media $\boldsymbol{\mu}$ y desviaci贸n est谩ndar $\boldsymbol{\sigma}$ en dos partes:

1.  **Una variable estoc谩stica fija:** Un vector de ruido $\boldsymbol{\epsilon}$ que se muestrea de una **distribuci贸n conocida y fija** (la Gaussiana est谩ndar, $\mathcal{N}(0, 1)$).
2.  **Una transformaci贸n determinista:** Una operaci贸n que convierte $\boldsymbol{\epsilon}$ en $\mathbf{z}$ utilizando los par谩metros del codificador ($\boldsymbol{\mu}$ y $\boldsymbol{\sigma}$).

La nueva ecuaci贸n para $\mathbf{z}$ es:

$$\mathbf{z} = \boldsymbol{\mu} + \boldsymbol{\sigma} \odot \boldsymbol{\epsilon}$$



**Efecto en la Retropropagaci贸n:**

* **Ruta del Gradiente:** Ahora, los gradientes pueden fluir desde $\mathbf{z}$ a trav茅s de la multiplicaci贸n y suma hacia $\boldsymbol{\mu}$ y $\boldsymbol{\sigma}$.
* **Aislamiento:** La fuente de la aleatoriedad, $\boldsymbol{\epsilon}$, no tiene par谩metros que necesiten ser aprendidos, por lo que el gradiente no necesita pasar a trav茅s del proceso de muestreo. Solo los par谩metros deterministas ($\boldsymbol{\mu}$ y $\boldsymbol{\sigma}$) reciben gradientes, lo que permite el ajuste de los pesos del codificador para modificar la distribuci贸n latente.

---

## 锔 VAE vs. GAN: Comparativa de Modelos Generativos

Los **Autoencoders Variacionales (VAE)** y las **Redes Generativas Antag贸nicas (GAN)** son los dos modelos generativos m谩s influyentes de la 煤ltima d茅cada, pero difieren fundamentalmente en su objetivo, entrenamiento y resultados.

### VAE (Autoencoder Variacional)

| Caracter铆stica | Descripci贸n |
| :--- | :--- |
| **Fundamento** | Probabil铆stico/Bayesiano. Maximiza el L铆mite Inferior de la Evidencia (ELBO). |
| **Arquitectura** | Un solo modelo con dos partes: **Codificador** (Inferencia) y **Decodificador** (Generaci贸n). |
| **Entrenamiento** | **Directo** (entrenan el Codificador y el Decodificador simult谩neamente). |
| **P茅rdida Principal** | La p茅rdida de reconstrucci贸n se optimiza junto con la Divergencia KL. |
| **Generaci贸n** | **F谩cil y controlable.** El espacio latente $\mathbf{z}$ es continuo y estructurado (suave), facilitando la interpolaci贸n y manipulaci贸n de atributos. |
| **Calidad de Salida** | Tiende a producir resultados m谩s **borrosos** o suavizados, ya que optimiza una p茅rdida de reconstrucci贸n media basada en verosimilitud (minimiza la distancia a la distribuci贸n real). |
| **Prop贸sito** | Aprendizaje de representaciones, inferencia, manipulaci贸n de atributos. |

### GAN (Red Generativa Antag贸nica)

| Caracter铆stica | Descripci贸n |
| :--- | :--- |
| **Fundamento** | Teor铆a de juegos y aprendizaje adversario (*Adversarial Learning*). |
| **Arquitectura** | Dos modelos en competencia: **Generador** ($G$) y **Discriminador** ($D$). |
| **Entrenamiento** | **Adversario** (minimax). El Generador intenta enga帽ar al Discriminador; el Discriminador intenta distinguir las muestras reales de las falsas. |
| **P茅rdida Principal** | El Generador se entrena en base al error del Discriminador. No hay t茅rmino expl铆cito de reconstrucci贸n. |
| **Generaci贸n** | **Dif铆cil de controlar.** El espacio latente suele ser m谩s desordenado, lo que dificulta la interpolaci贸n lineal sin producir resultados inv谩lidos. |
| **Calidad de Salida** | Tiende a generar resultados **m谩s n铆tidos y realistas**, ya que el Discriminador castiga las salidas borrosas o no naturales. |
| **Prop贸sito** | Generaci贸n de muestras fotorealistas, s铆ntesis de im谩genes. |

### Conclusi贸n de la Comparativa

| Criterio | GAN | VAE |
| :--- | :--- | :--- |
| **Generaci贸n de Imagen de Alta Calidad** | $\text{GAN} \gg \text{VAE}$ |
| **Control sobre el Espacio Latente** | $\text{VAE} \gg \text{GAN}$ |
| **Estabilidad de Entrenamiento** | $\text{VAE} \gg \text{GAN}$ |
| **Inferencia (*Encoding*)** | $\text{VAE}$ lo permite intr铆nsecamente ($\mathbf{x} \to \mathbf{z}$); $\text{GAN}$ requiere un modelo de inferencia separado. |

En esencia, el VAE sacrifica una peque帽a cantidad de calidad de salida a cambio de una **estructura latente m谩s interpretable** y un **entrenamiento m谩s estable**. El GAN, por otro lado, busca la excelencia visual a trav茅s de la competencia, lo que a menudo lo hace inestable y sin una forma nativa de mapear datos reales a c贸digos latentes.


---

Continua: [[3-2](https://github.com/webmastervetea/advanced-ai-curriculum/blob/main/temarios/3-2-redes-generativas-adversarias.md)] 
