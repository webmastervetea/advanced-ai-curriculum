
##  Desbloqueando Secuencias Largas: Estrategias para la Atenci贸n Lineal en Transformers

El mecanismo de Auto-Atenci贸n en los *Transformers* originales calcula las interacciones entre cada *token* de la secuencia y **todos** los dem谩s *tokens*. Para una secuencia de longitud $N$, esto requiere $N \times N$ operaciones, resultando en una complejidad computacional de $O(N^2)$ en tiempo y memoria.

Las arquitecturas modernas abordan esto mediante la introducci贸n de **patrones de atenci贸n dispersos** y **m茅todos de aproximaci贸n**.

---

### 1. El Problema Cuadr谩tico ($O(N^2)$)

Cuando la longitud de la secuencia $N$ es grande (por ejemplo, $N=4096$ en lugar de $N=512$), la complejidad cuadr谩tica se vuelve inviable:

| Longitud de Secuencia ($N$) | Complejidad ($N^2$) | Consumo de Memoria ($O(N^2)$) |
| :---: | :---: | :---: |
| 512 | $\approx 262,000$ | Peque帽o (Est谩ndar) |
| 4,096 | $\approx 16.7$ millones | Grande (Requiere m谩s GPU VRAM) |
| 32,768 (Documento) | $\approx 1.07$ mil millones | Inviable (Fuera de memoria) |

---

### 2. Atenci贸n Dispersa (*Sparse Attention*): Patrones Predefinidos

En lugar de que cada *Query* ($Q$) atienda a todos los *Keys* ($K$), la atenci贸n dispersa restringe las interacciones a un subconjunto predefinido, forzando la complejidad a ser $O(N \cdot \sqrt{N})$ o $O(N)$.

#### A. Atenci贸n Localizada (*Sliding Window Attention*)

* **Principio:** Los *tokens* solo atienden a sus **vecinos inmediatos** dentro de una ventana fija de tama帽o $W$.
* **Mecanismo:** Un *token* en la posici贸n $i$ solo calcula la atenci贸n con los *tokens* en el rango $[i-W, i+W]$.
* **Ventaja:** Reduce la complejidad a $O(N \cdot W)$, lo que es $O(N)$ si $W$ es constante.
* **Limitaci贸n:** Pierde la capacidad de modelar **dependencias a muy largo plazo** entre *tokens* separados por m谩s de $W$ posiciones.

#### B. Atenci贸n Dilatada y Global (*Dilated and Global Attention*)
Arquitecturas como **LongFormer** combinan varios patrones para mitigar la p茅rdida de contexto lejano:

1.  **Atenci贸n Local:** La atenci贸n de ventana deslizante mencionada arriba.
2.  **Atenci贸n Dilatada:** Simula una ventana de atenci贸n m谩s grande al **saltar** *tokens* intermedios (similar a las CNNs dilatadas). Esto aumenta el campo de visi贸n sin aumentar el costo computacional.
3.  **Atenci贸n Global:** Se designan un peque帽o n煤mero de *tokens* (ej., el `[CLS]` token o tokens espec铆ficos del problema) para atender a **toda la secuencia**.
    * **Beneficio:** Estos *tokens* globales act煤an como centros de informaci贸n, permitiendo que la informaci贸n a larga distancia fluya a trav茅s de ellos.
    * **Complejidad:** Se mantiene cerca de $O(N)$.

---

### 3. T茅cnicas de Aproximaci贸n: Eliminando la Matriz Cuadr谩tica

Estos m茅todos buscan evitar el c谩lculo expl铆cito y el almacenamiento de la matriz de atenci贸n $N \times N$, que es el coraz贸n del problema de la memoria y el tiempo.

#### A. Atenci贸n basada en N煤cleos (*Kernel-based Attention*)

* **Principio:** Si la funci贸n *softmax* de la atenci贸n puede aproximarse utilizando funciones de n煤cleo (*kernels*), la multiplicaci贸n de matrices en la capa de atenci贸n se puede reordenar para **no depender de $N$**.
* **Mecanismo (Ejemplo: Reformer/Linformer):** El c谩lculo de atenci贸n $(\text{softmax}(\mathbf{Q}\mathbf{K}^T)\mathbf{V})$ se convierte en una multiplicaci贸n reordenada:
    $$\mathbf{O} = \mathbf{Q} \cdot (\mathbf{K}^T \mathbf{V})$$
    Usando una aproximaci贸n de n煤cleo, se puede reordenar la operaci贸n de tal manera que $\mathbf{K}^T \mathbf{V}$ sea una matriz m谩s peque帽a, reduciendo la complejidad a $O(N)$.
* **Ventaja:** Logra una complejidad estrictamente $O(N)$ sin sacrificar la capacidad de modelar dependencias lejanas.

#### B. Atenci贸n con Memoria Condensada (*Compacted Memory Attention*)

Arquitecturas como **Performer** utilizan t茅cnicas basadas en la proyecci贸n de caracter铆sticas aleatorias:

* **Mecanismo:** En lugar de calcular el producto $\mathbf{Q}\mathbf{K}^T$, los vectores de *Query* ($\mathbf{Q}$) y *Key* ($\mathbf{K}$) se mapean a un espacio de menor dimensi贸n mediante **Funciones de Caracter铆sticas Aleatorias Positivas (PosFeats)**. Esto permite una aproximaci贸n probabil铆stica de la funci贸n *softmax*.
* **Beneficio:** Evita el cuello de botella de la matriz de atenci贸n y permite que la complejidad de la atenci贸n sea $O(N)$.

---

### 4. T茅cnicas con Memoria Recurrente

Para secuencias extremadamente largas, el modelo necesita una forma de **resumir** el pasado y llevar ese resumen al presente sin procesar todo el historial.

#### A. Transformadores Recurrentes (*Recurrent Transformers*)

* **Mecanismo:** El modelo procesa la secuencia en bloques (chunks). Despu茅s de procesar el bloque $i$, genera un vector de **memoria condensada** que resume la informaci贸n clave de ese bloque. Este vector de memoria se utiliza como una entrada *Key/Value* especial para el procesamiento del bloque $i+1$.
* **Arquitecturas (Ejemplo: Transformer-XL):** La atenci贸n en el bloque $i+1$ se calcula con los *tokens* del bloque actual *m谩s* los *tokens* de memoria condensada del bloque $i$. 
* **Ventaja:** Permite que el modelo mantenga un estado de memoria coherente durante secuencias te贸ricamente infinitas, manteniendo la complejidad de cada paso en $O(N)$.

### Conclusi贸n

La b煤squeda de la atenci贸n lineal o cuasilineal es el motor de la investigaci贸n en *Transformers* de pr贸xima generaci贸n. Mientras que los patrones de **Atenci贸n Dispersa** ofrecen soluciones pr谩cticas y probadas (como LongFormer) al restringir las interacciones, las t茅cnicas de **Aproximaci贸n por N煤cleos** (como Performer) y los **Modelos Recurrentes** (como Transformer-XL) prometen soluciones escalables que pueden procesar documentos completos, secuencias de ADN y conjuntos de datos de series temporales masivas, abriendo la puerta a aplicaciones previamente imposibles.

---

Continua: [[30.3.1](https://github.com/webmastervetea/advanced-ai-curriculum/blob/main/temarios/30-3-1.md)] 
