# üåê Entrenamiento Distribuido: Escalando Modelos Grandes con Paralelismo

El **Entrenamiento Distribuido** consiste en dividir el proceso computacional y de memoria de un modelo de *Deep Learning* entre m√∫ltiples dispositivos o nodos (GPUs, TPUs, CPUs) conectados en red. Esto es necesario cuando el **tama√±o del modelo** (cientos de miles de millones de par√°metros) o el **tama√±o del lote (*batch size*)** requerido para una convergencia estable exceden la capacidad de memoria o procesamiento de un √∫nico acelerador.

## 1. Paralelismo de Datos (*Data Parallelism*)

El **Paralelismo de Datos** es la forma m√°s com√∫n y simple de entrenamiento distribuido. Se utiliza cuando el **modelo completo cabe en la memoria** de una sola unidad de procesamiento, pero se necesita un **lote de datos m√°s grande** o una mayor velocidad de entrenamiento.

### A. Mecanismo B√°sico

1.  **R√©plicas del Modelo:** Se crea una **copia id√©ntica** y completa del modelo ($\theta$) en cada uno de los $N$ dispositivos (llamados *workers* o nodos).
2.  **Divisi√≥n del Lote:** El lote de datos global se divide equitativamente en $N$ sub-lotes (*mini-batches*). Cada dispositivo procesa su propio sub-lote de forma **independiente**.
3.  **C√°lculo de Gradientes:** Cada dispositivo calcula el pase hacia adelante y hacia atr√°s, obteniendo sus propios **gradientes locales** ($\nabla_i$) basados en su sub-lote.
4.  **Sincronizaci√≥n (All-Reduce):** Los gradientes locales se **agregan y promedian** entre todos los dispositivos para obtener un gradiente global que representa el paso de actualizaci√≥n. Esta comunicaci√≥n de gradientes es el punto de contenci√≥n principal de la latencia.
5.  **Actualizaci√≥n:** El optimizador utiliza el gradiente promedio para actualizar los pesos en cada dispositivo, asegurando que todos los modelos permanezcan **sincronizados** al comienzo del siguiente paso.



### B. Tipos de Sincronizaci√≥n

* **Sincr√≥nico:** Todos los dispositivos deben esperar a que el dispositivo m√°s lento termine su c√°lculo y sincronice los gradientes antes de proceder al siguiente paso. Garantiza una convergencia id√©ntica al entrenamiento con un solo dispositivo.
* **Asincr√≥nico:** Los dispositivos actualizan los pesos del modelo maestro de forma independiente tan pronto como terminan. Esto puede ser m√°s r√°pido, pero puede llevar a una **convergencia inestable** debido a que algunos gradientes pueden basarse en pesos ya desactualizados.

---

## 2. Paralelismo de Modelos (*Model Parallelism*)

El **Paralelismo de Modelos** es esencial cuando el **modelo en s√≠ mismo es demasiado grande** para que sus par√°metros, gradientes y estados del optimizador quepan en la memoria de un solo dispositivo (el caso t√≠pico de los LLMs con cientos de miles de millones de par√°metros).

El modelo se divide en partes, y cada parte se coloca en un dispositivo diferente.

### A. Paralelismo de Pipeline (*Pipeline Parallelism*)

Esta t√©cnica divide el modelo **verticalmente** a lo largo de sus capas, creando una l√≠nea de ensamblaje (*pipeline*).

1.  **Divisi√≥n por Capas:** Las capas del modelo se dividen en $K$ etapas (particiones), y cada etapa se asigna a una GPU/TPU diferente.
2.  **Flujo de Datos:** La GPU 1 calcula las primeras capas y pasa las **activaciones** a la GPU 2. La GPU 2 calcula sus capas y pasa las activaciones a la GPU 3, y as√≠ sucesivamente.
3.  **Desaf√≠o:** La ejecuci√≥n secuencial pura causa que las GPUs permanezcan inactivas esperando la entrada de la GPU anterior. Esto se resuelve dividiendo el lote en **micro-lotes** y ejecutando el *forward* y *backward pass* de manera intercalada para mantener el *pipeline* lleno.



### B. Paralelismo de Tensor (*Tensor Parallelism* o *Intra-Layer Parallelism*)

Esta t√©cnica divide las grandes operaciones tensoriales **horizontalmente** dentro de una misma capa. Se utiliza cuando incluso las **matrices de pesos de una sola capa son demasiado grandes** para un solo dispositivo.

1.  **Divisi√≥n de Matrices:** Las matrices clave de la capa (t√≠picas de la Atenci√≥n o *Feed-Forward* en un Transformador) se dividen en fragmentos y cada fragmento se asigna a una GPU.
2.  **Comunicaci√≥n Intensa:** Requiere una comunicaci√≥n de red de muy alta velocidad, ya que los dispositivos deben intercambiar resultados intermedios (*All-Gather*, *Reduce-Scatter*) antes de que se pueda completar la operaci√≥n de la capa.

---

## 3. Estrategias H√≠bridas y Otros M√©todos de Eficiencia

El entrenamiento de modelos de vanguardia (ej. GPT-4, Llama 3) utiliza una combinaci√≥n sofisticada de todas estas t√©cnicas.

### A. Estrategia H√≠brida (Modelo y Datos)

El enfoque m√°s com√∫n para los LLMs es la **Hibridaci√≥n del Paralelismo**:

1.  **Paralelismo de Modelo (Tensor/Pipeline):** Se divide el modelo original entre $M$ dispositivos para que quepa en la memoria.
2.  **Paralelismo de Datos:** Se replica esta configuraci√≥n de $M$ dispositivos $D$ veces, creando un cl√∫ster de $M \times D$ dispositivos. El *batch* de datos se divide entre las $D$ r√©plicas.

### B. Paralelismo del Optimizador (ZeRO)

A medida que los modelos crecen, los pesos del modelo (par√°metros), los gradientes y el **estado del optimizador** (ej. los momentos de Adam) consumen una cantidad masiva de memoria. Por ejemplo, el estado del optimizador por s√≠ solo puede ocupar 12 veces la memoria de los pesos originales.

**ZeRO** (*Zero Redundancy Optimizer*) es una t√©cnica clave que aborda esto dividiendo y distribuyendo:

1.  **ZeRO-1:** Distribuye el estado del optimizador entre todos los *workers*.
2.  **ZeRO-2:** Distribuye el estado del optimizador **y** los gradientes.
3.  **ZeRO-3:** Distribuye el estado del optimizador, los gradientes **y los propios par√°metros del modelo**.

Esto permite entrenar modelos con miles de millones de par√°metros con mucho menos hardware por nodo, ya que el modelo en s√≠ nunca existe por completo en una sola GPU.

### C. Activaciones de Punto de Verificaci√≥n (*Activation Checkpointing*)

Durante el *backward pass*, el c√°lculo de gradientes requiere las **activaciones** del *forward pass*. Para modelos profundos, almacenar todas estas activaciones consume gran parte de la memoria.

El *Activation Checkpointing* resuelve esto al **almacenar solo las activaciones de unas pocas capas seleccionadas** (los *checkpoints*). Las activaciones de las capas intermedias se **recalculan** durante el *backward pass* solo cuando es necesario. Esto ahorra una gran cantidad de memoria a costa de un ligero aumento en el tiempo de c√°lculo.

Al combinar el Paralelismo de Datos, las estrategias de Paralelismo de Modelos y las t√©cnicas de eficiencia de memoria como ZeRO y el *Checkpointing*, los investigadores e ingenieros pueden superar las limitaciones f√≠sicas del hardware y entrenar los sistemas de IA m√°s avanzados del mundo.


---

Continua: [[7-2-1](https://github.com/webmastervetea/advanced-ai-curriculum/blob/main/temarios/7-2-1-optimizacion-latencia-rendimiento-produccion.md)] 
