

##  Modelado de Datos Faltantes: Imputaci贸n con *Deep Learning*

La imputaci贸n con *Deep Learning* se basa en la idea de que la red neuronal puede aprender la **distribuci贸n de probabilidad conjunta** de todos los datos observados, y luego utilizar esa distribuci贸n para generar muestras plausibles para los valores faltantes.

### 1. El Desaf铆o de la Imputaci贸n

* **Supuesto Limitado:** Los m茅todos tradicionales (ej. la imputaci贸n por media o mediana) asumen que los datos faltantes son **completamente al azar (*MCAR*)** o solo dependen de las variables observadas (*MAR*), y no capturan la verdadera complejidad de la relaci贸n entre variables.
* **Sesgo:** La imputaci贸n simple introduce un sesgo al reducir la varianza de los datos o al subestimar la incertidumbre.

### 2. Autoencoders Denoising (DAE) y *Masking*

Los *Autoencoders Denoising* son uno de los primeros y m谩s intuitivos usos del *Deep Learning* para la imputaci贸n.

* **Mecanismo de Entrenamiento:**
    1.  Se toma el conjunto de datos completo y se introduce ruido artificialmente (similar a c贸mo ocurre el dato faltante en el mundo real). Esto se hace aplicando una **m谩scara binaria** a las entradas (*masking*).
    2.  El *Autoencoder* se entrena para reconstruir la **entrada original no ruidosa** a partir de la entrada ruidosa.
* **Mecanismo de Imputaci贸n:** Una vez entrenado, el *Autoencoder* recibe un registro con valores faltantes (que act煤a como la entrada ruidosa) y la salida del *decoder* es la versi贸n reconstruida con los valores faltantes **imputados plausibles**.



### 3. Modelos Generativos para Imputaci贸n Plausible

Los modelos generativos son el m茅todo m谩s avanzado, ya que pueden capturar la distribuci贸n de probabilidad de los datos, permitiendo la **Imputaci贸n M煤ltiple** (generar varias versiones plausiblemente diferentes del dato faltante).

#### A. Redes Generativas Adversarias para Imputaci贸n (GAIN)
GAIN (*Generative Adversarial Imputation Networks*) aplica el marco GAN para la imputaci贸n.

* **Arquitectura:**
    1.  **Generador ($G$):** Recibe el dato incompleto y una m谩scara de ruido. Genera una imputaci贸n plausible para los valores faltantes.
    2.  **Discriminador ($D$):** Intenta distinguir si un valor imputado proviene del generador o es un valor observado originalmente en el *dataset*.
* **Funci贸n:** El Generador se entrena para **enga帽ar** al Discriminador creando imputaciones que no se pueden distinguir de los datos reales. Esto obliga al Generador a aprender la verdadera distribuci贸n de probabilidad subyacente de los datos, resultando en imputaciones de alta calidad y realismo.

#### B. Imputaci贸n con *Variational Autoencoders* (VAEs)
Los VAEs se utilizan para capturar la incertidumbre del dato faltante.

* **Mecanismo:** El VAE aprende una distribuci贸n probabil铆stica en el espacio latente. Cuando imputa un valor faltante, en lugar de generar un solo punto determinista (como el DAE), genera una **distribuci贸n de posibles valores** para el dato faltante.
* **Ventaja:** Esto permite cuantificar la **incertidumbre de la imputaci贸n**, que es vital en el diagn贸stico o la toma de decisiones, superando una de las grandes cr铆ticas de los m茅todos de imputaci贸n simple.

### 4. La Importancia del *Masking* y el *Feature Encoding*

Para que la imputaci贸n con DL funcione, el modelo debe saber **qu茅 valores faltan**.

* **Codificaci贸n de la M谩scara:** Junto con el vector de datos incompleto, el modelo siempre recibe un **vector de m谩scara binaria** (donde 1 indica observado, 0 indica faltante). Esto permite al modelo utilizar la informaci贸n de la m谩scara como una *feature* de entrada.
* **Codificaci贸n de Datos Categ贸ricos y Temporales:** Los datos heterog茅neos y temporales (como los vistos en el IoT) deben ser codificados correctamente antes de la imputaci贸n (ej. *embeddings* para categor铆as, *LSTMs* para secuencias temporales).

---

### Conclusi贸n

La **Imputaci贸n con *Deep Learning***, particularmente a trav茅s de **Redes Generativas Adversarias (GAIN)** y **VAEs**, supera los m茅todos estad铆sticos tradicionales al capturar las complejas interdependencias no lineales de los datos. Al transformar la tarea de imputaci贸n en un problema de **aprendizaje de la distribuci贸n de probabilidad**, estos modelos pueden generar valores faltantes que no solo son plausibles, sino que tambi茅n reflejan la verdadera incertidumbre del conjunto de datos.
---

Continua: [[55.2](https://github.com/webmastervetea/advanced-ai-curriculum/blob/main/temarios/55-2.md)] 
