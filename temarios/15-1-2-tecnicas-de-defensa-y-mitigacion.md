# üõ°Ô∏è Defensas y Mitigaci√≥n: Blindando Modelos contra Ataques Adversarios

Los **Ejemplos Adversarios** representan una amenaza cr√≠tica para la seguridad e integridad de los sistemas de *Deep Learning* en aplicaciones sensibles. Las **T√©cnicas de Defensa y Mitigaci√≥n** son esenciales para aumentar la robustez de los modelos y garantizar que se comporten de manera predecible y fiable ante *inputs* maliciosos.

Las estrategias de defensa se dividen generalmente en tres categor√≠as: **Modificaci√≥n del Entrenamiento**, **Modificaci√≥n del Modelo** y **Detecci√≥n de Entrada**.

## 1. Modificaci√≥n del Entrenamiento: Entrenamiento Adversario

El **Entrenamiento Adversario (*Adversarial Training*)** es la t√©cnica de defensa m√°s efectiva y considerada el est√°ndar de oro en la lucha contra los ataques adversarios. En lugar de cambiar la arquitectura del modelo, cambia el proceso por el cual el modelo aprende.

### A. Mecanismo de Funcionamiento

El entrenamiento adversario opera mediante un ciclo iterativo de dos pasos, similar a un juego min-max:

1.  **Generaci√≥n de Ejemplos Adversarios:** En cada paso de entrenamiento (o cada pocas √©pocas), se genera un nuevo lote de **Ejemplos Adversarios** ($\mathbf{x}_{\text{adv}}$) para los datos del lote actual ($\mathbf{x}$). Esto generalmente se realiza utilizando un ataque **White-Box** fuerte y eficiente, como **PGD (*Projected Gradient Descent*)**.
2.  **Entrenamiento y Refuerzo:** El modelo se entrena para **clasificar correctamente** no solo los datos originales ($\mathbf{x}$), sino tambi√©n sus correspondientes versiones adversarias ($\mathbf{x}_{\text{adv}}$). La funci√≥n de p√©rdida se calcula sobre una mezcla de datos normales y adversarios.

### B. Beneficios y Desaf√≠os

| Aspecto | Descripci√≥n |
| :--- | :--- |
| **Robustez Local** | El entrenamiento adversario **suaviza los l√≠mites de decisi√≥n** del modelo en las vecindades cercanas a los datos de entrenamiento, haciendo que el modelo sea intr√≠nsecamente m√°s resistente a peque√±as perturbaciones. |
| **Costo Computacional** | Es **muy costoso** computacionalmente. Generar ejemplos adversarios por gradiente en cada paso de entrenamiento aumenta el tiempo de entrenamiento en un factor significativo (a menudo 5x a 10x). |
| **Generalizaci√≥n** | Tiende a mejorar la robustez contra el tipo espec√≠fico de ataque utilizado en el entrenamiento (ej. si se entrena con FGSM, es robusto a FGSM). Es deseable usar ataques iterativos fuertes como PGD para la generalizaci√≥n. |

---

## 2. Detecci√≥n de Entrada At√≠pica (*Outlier Detection*)

Los m√©todos de detecci√≥n no intentan hacer que el modelo clasifique el *input* adversario correctamente, sino que intentan **identificar la entrada perturbada** antes de que llegue al clasificador. Si el sistema detecta un *input* adversario con alta probabilidad, puede rechazarlo o dirigirlo a un clasificador m√°s robusto o a un experto humano.

### A. Detecci√≥n por Diferencias en las Activaciones

Los ejemplos adversarios a menudo residen en regiones de baja probabilidad de la distribuci√≥n de datos original.

* **Mecanismo:** Se utiliza la capa de **Activaciones Internas** del modelo. Se entrena un clasificador binario simple (ej. una M√°quina de Soporte Vectorial, SVM) para distinguir entre las activaciones de entradas leg√≠timas y las activaciones de entradas adversarias.
* **Principio:** Aunque la salida final (la predicci√≥n) del modelo se ve afectada por el ataque, el patr√≥n de activaci√≥n en las capas intermedias puede ser notablemente diferente entre un dato normal y un dato ruidoso.

### B. Detecci√≥n por Espacio de *Embedding*

Se eval√∫a la distancia del *input* respecto a la distribuci√≥n de los datos de entrenamiento.

* **Mecanismo:** El *input* se proyecta a un espacio de *embedding* (una capa latente). Si la distancia del *input* a su vecino m√°s cercano en el conjunto de entrenamiento es anormalmente grande (utilizando m√©tricas como el **Score de Mahalanobis**), se etiqueta como at√≠pico.
* **Ventaja:** Detecta *inputs* que est√°n fuera de la distribuci√≥n del entrenamiento (OOD) y que son un subproducto com√∫n de los ataques adversarios.

---

## 3. Otras Estrategias de Mitigaci√≥n

### A. Gradiente Oculto (*Gradient Masking/Obfuscation*)

Esta es una defensa que busca hacer que el ataque **White-Box** falle.

* **Mecanismo:** El modelo se modifica para tener un gradiente inutilizable o nulo en los puntos de decisi√≥n cr√≠ticos (ej. usando funciones no diferenciables o cuantificaci√≥n).
* **Desaf√≠o:** Aunque inicialmente parece exitoso, los investigadores adversarios han demostrado que el **enmascaramiento de gradiente es una defensa d√©bil**. Los atacantes pueden eludirlo usando ataques basados en la transferencia (*Black-Box*) o gradientes aproximados.

### B. Procesamiento de Entrada (*Input Preprocessing*)

Los *inputs* se "limpian" antes de ser alimentados al modelo.

* **Mecanismo:** Se aplican t√©cnicas de reducci√≥n de ruido (ej. **Filtrado Gaussiano**, **Reducci√≥n Total de la Variaci√≥n**) o se utilizan m√©todos basados en *Autoencoders* para eliminar la perturbaci√≥n imperceptible.
* **Desaf√≠o:** Puede ser vulnerable si el atacante conoce el preprocesamiento y dise√±a el ruido para evadirlo.

## 4. Conclusi√≥n

La **Robustez Adversaria** no es un estado binario, sino un continuo. El **Entrenamiento Adversario** es la base m√°s s√≥lida para construir modelos resistentes, pero a menudo debe combinarse con **T√©cnicas de Detecci√≥n de Entradas At√≠picas** para manejar *inputs* que est√°n completamente fuera de la distribuci√≥n de entrenamiento y para mitigar ataques *Black-Box* m√°s complejos. La investigaci√≥n en este campo es una carrera armament√≠stica constante entre atacantes y defensores.


---

Continua: [[15-2-1]()] 
