
##  XAI: M茅todos Locales vs. Globales para Desvelar el Comportamiento del Modelo

La comprensibilidad de un modelo de *Deep Learning* requiere una doble perspectiva: **local** (驴por qu茅 esta predicci贸n para este punto de datos?) y **global** (驴c贸mo toma el modelo decisiones en general?). Los m茅todos locales (**LIME, SHAP**) proporcionan granularidad, mientras que los m茅todos globales ofrecen una visi贸n de alto nivel crucial para la auditor铆a, la detecci贸n de sesgos y la confianza general en el sistema.

---

### 1. M茅todos Locales: Explicando una Predicci贸n nica

Los m茅todos locales se centran en el **punto de inflexi贸n de la decisi贸n** para una instancia espec铆fica. Responden a la pregunta: "Para esta entrada 煤nica, 驴cu谩l es la influencia de cada caracter铆stica en la salida del modelo?"

#### A. LIME (Local Interpretable Model-agnostic Explanations)

* **Principio:** LIME asume que, aunque un modelo de *Deep Learning* sea globalmente complejo, su comportamiento puede aproximarse de forma sencilla **localmente** (en la vecindad de un solo punto de datos).
* **Mecanismo:**
    1.  **Perturbaci贸n:** Genera m煤ltiples instancias de datos **sint茅ticos** cerca de la instancia a explicar (perturbando sus caracter铆sticas).
    2.  **Etiquetado:** Utiliza el modelo complejo original (la "caja negra") para obtener la predicci贸n para cada una de estas instancias perturbadas.
    3.  **Modelo Sustituto Local:** Entrena un modelo **interpretable** (p. ej., regresi贸n lineal o 谩rboles de decisi贸n) utilizando las instancias perturbadas y sus predicciones.
    4.  **Explicaci贸n:** La explicaci贸n es la ponderaci贸n de las caracter铆sticas del modelo simple.
* **Ventaja:** Es totalmente **agn贸stico al modelo** (*model-agnostic*) y produce explicaciones visualmente intuitivas (p. ej., superponiendo las caracter铆sticas importantes en una imagen).
* **Limitaci贸n:** La calidad de la explicaci贸n depende de c贸mo se define la "vecindad" local y de la fidelidad del modelo sustituto a esa vecindad.

#### B. SHAP (SHapley Additive exPlanations)

* **Principio:** Basado en la teor铆a de juegos cooperativos, SHAP asigna un valor de contribuci贸n justo a cada caracter铆stica, midiendo su **impacto marginal promedio** en la predicci贸n.
* **Mecanismo:**
    1.  **Valoraci贸n de Shapley:** El valor de Shapley de una caracter铆stica es su contribuci贸n marginal promedio a la predicci贸n final, calculada sobre **todas las posibles combinaciones (coaliciones)** de caracter铆sticas.
    2.  **Alineaci贸n:** SHAP garantiza la **consistencia** (si un modelo cambia para que una caracter铆stica tenga un mayor impacto, su valor SHAP aumenta) y la **aditividad** (la suma de los valores SHAP de todas las caracter铆sticas es igual a la diferencia entre la predicci贸n y el valor base).
* **Ventaja:** Es el 煤nico m茅todo de XAI que posee una s贸lida base te贸rica, ofreciendo una distribuci贸n "justa" de la influencia de las caracter铆sticas.
* **Integraci贸n Local/Global:** Los valores SHAP pueden sumarse y visualizarse a escala global, convirti茅ndolos en un potente puente entre la explicabilidad local y la global.

---

### 2. M茅todos Globales: Comprensi贸n del Comportamiento General

Los m茅todos globales buscan resumir c贸mo el modelo complejo toma decisiones en todo el espacio de caracter铆sticas. Son esenciales para el **descubrimiento de sesgos** y la **validaci贸n del modelo**.

#### A. Importancia Global de Caracter铆sticas (SHAP Global Summaries)

El m茅todo m谩s simple consiste en agregar las contribuciones locales.

* **Mecanismo:** Se calcula la magnitud promedio del valor de contribuci贸n de cada caracter铆stica a trav茅s de un gran *dataset* de prueba.
* **Visualizaci贸n:** Gr谩ficos de barras que muestran qu茅 caracter铆sticas (ej., "ingresos", "edad") son, en promedio, las m谩s importantes para las predicciones del modelo.
* **Ventaja:** F谩cil de implementar y proporciona una vista r谩pida de alto nivel.
* **Limitaci贸n:** Solo muestra la importancia, **no la direcci贸n** de la influencia (p. ej., 驴los ingresos altos aumentan o disminuyen la probabilidad de la predicci贸n?).

#### B. Gr谩ficos de Dependencia Parcial (Partial Dependence Plots, PDP)

* **Principio:** Muestra la relaci贸n marginal (promedio) entre una o dos caracter铆sticas y la predicci贸n de la salida.
* **Mecanismo:** Para una caracter铆stica de inter茅s (ej., "Edad"), se mantiene esa caracter铆stica fija en un valor espec铆fico (ej., 30 a帽os) y se promedian las predicciones del modelo para **todos** los dem谩s puntos de datos. Este proceso se repite para cada valor posible de la caracter铆stica.
* **Resultado:** Un gr谩fico que ilustra si la predicci贸n aumenta o disminuye (y c贸mo) a medida que cambia la caracter铆stica de inter茅s.
* **Ventaja:** Muestra la **direcci贸n y forma** de la influencia global (p. ej., "la predicci贸n aumenta hasta los 50 a帽os y luego disminuye").
* **Limitaci贸n:** Asume que la caracter铆stica de inter茅s es independiente de las otras caracter铆sticas, lo cual rara vez es cierto en datos reales.

#### C. Expectativas Condicionales Individuales (Individual Conditional Expectation, ICE)

* **Principio:** Aborda la limitaci贸n del PDP mostrando el efecto de la caracter铆stica en **cada instancia individual**, en lugar de promediar sobre el *dataset*.
* **Mecanismo:** Similar a PDP, pero se traza una l铆nea de predicci贸n para **cada punto de datos** en el *dataset*.
* **Resultado:** Un conjunto de l铆neas superpuestas. Si todas las l铆neas de ICE son similares al PDP, la relaci贸n es globalmente uniforme. Si las l铆neas se cruzan (lo que indica **interacciones** entre caracter铆sticas), el PDP podr铆a estar ocultando un comportamiento complejo.

#### D. Modelos Sustitutos Globales (*Global Surrogate Models*)

* **Principio:** Entrenar un modelo intr铆nsecamente interpretable (como un rbol de Decisi贸n o una Regla de Regresi贸n Simple) para **imitar** las predicciones del modelo de caja negra en todo el *dataset*.
* **Ventaja:** Si el modelo sustituto tiene alta fidelidad, su estructura (reglas simples) se convierte en una explicaci贸n global del comportamiento de la caja negra.

---

### 3. S铆ntesis: La Necesidad de la Fusi贸n Local-Global

La explicabilidad robusta requiere una sinergia entre ambos enfoques:

1.  **Vista Global (Auditor铆a):** Los PDP, ICE y los res煤menes SHAP ayudan a los ingenieros y auditores a detectar **sesgos sist茅micos** (p. ej., si la Edad tiene una influencia inesperada en el promedio).
2.  **Vista Local (Decisi贸n y Confianza):** LIME y SHAP se entregan al usuario o al tomador de decisiones para **justificar una predicci贸n espec铆fica** y proporcionar explicaciones contrafactuales.

Al combinar la solidez matem谩tica de SHAP con la visi贸n agregada de los PDP, y verificar estas tendencias con la sensibilidad individual de los gr谩ficos ICE, se puede construir una imagen completa y coherente del por qu茅 y c贸mo opera un sistema de IA.

---

Continua: [[28.3.1](https://github.com/webmastervetea/advanced-ai-curriculum/blob/main/temarios/28-3-1.md)] 
