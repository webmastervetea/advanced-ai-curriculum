##  Identificaci贸n de Grafos Causales: Descubriendo Causa y Efecto

El objetivo de la **Identificaci贸n de Grafos Causales** es transformar un conjunto de datos observacionales (no experimentales) en un **Grafo Ac铆clico Dirigido (DAG)**, donde los nodos son variables y las aristas dirigidas ($X \rightarrow Y$) representan una relaci贸n de causa y efecto.

### 1. Los Fundamentos: Limitaciones de la Correlaci贸n

La correlaci贸n se utiliza a menudo incorrectamente como evidencia de causalidad.

* **Confundidores (*Confounders*):** Un factor no observado ($Z$) que influye tanto en la causa ($X$) como en el efecto ($Y$), creando una correlaci贸n espuria (ej., $Z \rightarrow X$ y $Z \rightarrow Y$).
* **Causalidad Inversa:** $Y$ causa $X$, pero el an谩lisis de correlaci贸n sugiere $X$ causa $Y$.

El *Causal Discovery* busca resolver estos problemas bas谩ndose en la **Independencia Condicional** de las variables.

### 2. Marco Conceptual: Independencia Condicional (IC)

La IC es la prueba de fuego de la causalidad.

* **Definici贸n:** Dos variables $X$ y $Y$ son condicionalmente independientes dado un conjunto de variables $Z$ si, una vez que el valor de $Z$ se conoce, $X$ e $Y$ no tienen ninguna relaci贸n restante. Se escribe como $X \perp Y \mid Z$.
* **El Principio Causal Fiel (*Causal Faithfulness*):** Asume que todas las independencias condicionales observadas en los datos son el resultado de la estructura causal del DAG subyacente, y viceversa.
* **Separaci贸n D (*D-Separation*):** Es el m茅todo gr谩fico para determinar qu茅 independencias condicionales implica una estructura causal dada. Los algoritmos de descubrimiento causal invierten este proceso: usan las independencias observadas para inferir la estructura.

### 3. Algoritmos Basados en la Independencia Condicional

Estos algoritmos utilizan pruebas estad铆sticas de independencia condicional para inferir la estructura del grafo.

#### A. Algoritmo PC (Peter Clark)

El algoritmo PC es uno de los m茅todos m谩s conocidos y eficientes.

* **Mecanismo:**
    1.  **Inicializaci贸n:** Comienza con un grafo **completamente conectado y no dirigido** (asumiendo que todas las variables est谩n relacionadas inicialmente).
    2.  **Paso 1 (Remoci贸n de Aristas, Orden 0):** Para cada par $X, Y$, se prueba la independencia incondicional ($X \perp Y \mid \emptyset$). Si son independientes, la arista $X-Y$ se remueve.
    3.  **Paso 2 (Remoci贸n Condicional, Orden $k$):** Para los pares restantes, se prueba la independencia condicional dado un conjunto $Z$ de tama帽o creciente (de 1 a $k$). Si $X \perp Y \mid Z$, la arista $X-Y$ se remueve, y $Z$ se almacena como el conjunto que separa a $X$ e $Y$.
    4.  **Orientaci贸n:** Se utiliza la regla de los **Confluentes (*V-Structures*)** para determinar la direcci贸n de las aristas. Un confluente es una estructura $X \rightarrow Z \leftarrow Y$, donde $X$ e $Y$ son independientes, pero se vuelven **dependientes** al condicionar en $Z$ (su efecto com煤n).

#### B. Algoritmo *Fast Causal Inference* (FCI)
El algoritmo FCI se utiliza cuando se sospecha la presencia de **variables latentes no observadas** (confundidores no medidos).

### 4. Algoritmos Basados en la Puntuaci贸n (*Score-Based Methods*)

Estos algoritmos definen una m茅trica de "bondad de ajuste" para cualquier DAG y utilizan un algoritmo de b煤squeda para encontrar el DAG que maximiza la puntuaci贸n.

* **Mecanismo:**
    1.  Se define una puntuaci贸n $\text{Score}(G, D)$ que penaliza la complejidad del grafo ($G$) y premia la capacidad del grafo para ajustarse a los datos ($D$). La **Puntuaci贸n Bayesiana de Informaci贸n (BIC)** es com煤n.
    2.  Se utiliza una heur铆stica de b煤squeda (ej. *hill climbing*) para explorar el espacio de todos los posibles DAGs.
* **Desaf铆o:** El n煤mero de DAGs posibles crece super-exponencialmente con el n煤mero de variables, haciendo que la b煤squeda sea muy costosa.

### 5. Algoritmos Basados en Funciones de M谩quina (*Functional Causal Models, FCM*)

Estos modelos son 煤tiles cuando las relaciones causales se expresan mediante ecuaciones espec铆ficas, a menudo asumiendo **No Linealidad** o **Ruido No Gaussiano**.

* **Mecanismo:** Si $X \rightarrow Y$, la relaci贸n se expresa como $Y = f(X) + \epsilon$, donde $\epsilon$ es el t茅rmino de error o ruido.
* **Principio:** Se asume que el ruido $\epsilon$ es **independiente** de la causa $X$.
* **Ventaja:** En muchos casos, los FCM permiten determinar la direcci贸n causal 煤nicamente (ej., $X \rightarrow Y$ vs. $Y \rightarrow X$) en datos no gaussianos, algo que los m茅todos basados en IC a menudo no pueden hacer sin experimentaci贸n.



### 6. Aplicaciones en la Pr谩ctica

1.  **Biolog铆a de Sistemas:** Descubrir redes de regulaci贸n gen茅tica a partir de datos de expresi贸n g茅nica.
2.  **Econom铆a y Finanzas:** Identificar qu茅 indicadores econ贸micos son la verdadera causa (y no el efecto) de otros.
3.  **Rob贸tica:** Comprender el efecto de las acciones del robot en el entorno para una mejor planificaci贸n.

---

### Conclusi贸n

La **Identificaci贸n de Grafos Causales** permite a la IA ir m谩s all谩 de la correlaci贸n superficial. Mediante la aplicaci贸n de principios de **Independencia Condicional** (Algoritmo PC) y la **modelaci贸n funcional** del ruido (FCM), los algoritmos pueden inferir la direcci贸n y la estructura de las relaciones de causa y efecto. Este campo es vital para construir sistemas de IA que no solo predicen, sino que tambi茅n **razonan sobre el impacto de las intervenciones**.

---

Continua: [[46.2](https://github.com/webmastervetea/advanced-ai-curriculum/blob/main/temarios/46-2.md)] 
