
## ⚖️ Auditoría Algorítmica y Equidad: Medición y Mitigación de Sesgos

El sesgo algorítmico surge cuando los modelos de IA refuerzan o amplifican las disparidades existentes en los datos de entrenamiento (ej. sesgos históricos, representaciones insuficientes) y las proyectan en sus decisiones.

El proceso de auditoría requiere: **1) Definir métricas matemáticas de equidad, 2) Cuantificar la disparidad, y 3) Aplicar técnicas para reducirla.**

### 1. El Desafío: Múltiples Definiciones de Equidad

No existe una única definición matemática de "equidad". A menudo, las diferentes métricas de equidad son **mutuamente excluyentes**, lo que significa que maximizar una puede empeorar otra. Esto obliga a los auditores a tomar decisiones éticas sobre qué tipo de equidad priorizar.

Las métricas se basan en el concepto de **Grupos Protegidos o Sensibles** (ej. raza, género, edad, ingresos, etc.).

### 2. Métricas de Equidad Basadas en Distribución

Estas métricas buscan la igualdad de representación de los resultados, independientemente de la precisión.

#### A. Disparidad de Impacto (Disparate Impact)

* **Definición Sociológica:** El impacto de las decisiones del modelo debe ser proporcional entre los grupos protegidos.
* **Métrica Matemática (Regla del 4/5):** Mide la relación entre la tasa de selección del grupo con desventaja y la tasa de selección del grupo con ventaja.
    $$\text{DI} = \frac{P(\text{Selección} \mid \text{Grupo Desfavorecido})}{P(\text{Selección} \mid \text{Grupo Favorecido})}$$
* **Umbral:** Una Disparidad de Impacto por debajo del 80% (o 4/5) se considera evidencia de sesgo. Por ejemplo, si el grupo A es seleccionado el 10% de las veces y el grupo B solo el 5% de las veces, $\text{DI} = 5\% / 10\% = 0.5$, lo cual es una disparidad significativa.

#### B. Paridad Demográfica (Demographic Parity)

* **Definición:** La probabilidad de que un individuo sea seleccionado (clasificado como positivo) debe ser **independiente** de su pertenencia al grupo sensible $A$.
    $$P(\hat{Y}=1 \mid A=a) = P(\hat{Y}=1)$$
* **Problema:** Ignora la precisión. Un modelo puede lograr esta paridad simplemente seleccionando al azar en todos los grupos, lo que no es útil.

### 3. Métricas de Equidad Basadas en la Precisión

Estas métricas se centran en la igualdad de errores y la fiabilidad de las predicciones a través de los grupos.

#### A. Igualdad de Oportunidad (Equality of Opportunity)

* **Definición:** La tasa de verdaderos positivos (True Positive Rate, TPR) debe ser igual en todos los grupos sensibles. Solo se enfoca en la equidad para los individuos que **deberían** haber sido seleccionados (la "verdadera clase positiva").
    $$\text{TPR}_a = P(\hat{Y}=1 \mid Y=1, A=a)$$
* **Uso:** Esencial en decisiones de alto impacto donde los falsos negativos son costosos (ej. detección de enfermedades, admisión a la universidad).

#### B. Igualdad de Probabilidades (Equalized Odds)

* **Definición:** Una definición más estricta que exige la igualdad de **Tasa de Verdaderos Positivos (TPR)** **y** la igualdad de **Tasa de Falsos Positivos (FPR)** entre los grupos.
    $$\text{TPR}_a = \text{TPR}_b \quad \text{y} \quad \text{FPR}_a = \text{FPR}_b$$
* **Uso:** Importante en sistemas de justicia penal o de préstamo de alto riesgo, donde tanto los falsos positivos (castigo indebido) como los falsos negativos (oportunidad perdida) son inaceptables.



### 4. Estrategias de Mitigación de Sesgos

Las técnicas de mitigación se aplican en diferentes etapas del ciclo de vida del *Machine Learning*.

| Estrategia | Etapa de Aplicación | Mecanismo |
| :--- | :--- | :--- |
| **Pre-procesamiento** | Antes del Entrenamiento | **Re-muestreo/Ponderación:** Ajustar los datos de entrada para equilibrar la representación (ej. sobremuestrear el grupo desfavorecido). |
| **En el Modelo (*In-processing*)** | Durante el Entrenamiento | **Regularización de Adversarios:** Entrenar un clasificador adicional (adversario) para que no pueda predecir el atributo sensible $A$ a partir del *embedding* latente del modelo principal. |
| **Post-procesamiento** | Después del Entrenamiento | **Ajuste de Umbrales:** Aplicar diferentes umbrales de decisión a las puntuaciones del modelo para cada grupo sensible $A$ hasta que se cumpla la métrica de equidad deseada (ej. Igualdad de Oportunidad). |

### 5. Conclusión

La **Auditoría Algorítmica** requiere una elección consciente y rigurosa de las métricas de equidad, ya que la justicia es un concepto con múltiples definiciones. Métricas como la **Disparidad de Impacto** y la **Igualdad de Oportunidad** permiten cuantificar el sesgo de manera objetiva. La mitigación, utilizando técnicas como la **regularización adversaria** durante el entrenamiento o el **ajuste de umbrales** posterior, es crucial para construir sistemas de IA que no solo sean precisos, sino también éticos y equitativos en sus decisiones.

---

Continua: [[42.2](https://github.com/webmastervetea/advanced-ai-curriculum/blob/main/temarios/42-2.md)] 
