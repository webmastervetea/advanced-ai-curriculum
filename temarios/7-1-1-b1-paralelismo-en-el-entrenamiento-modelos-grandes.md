# 游깷 Paralelismo en el Entrenamiento de Modelos Grandes

El entrenamiento distribuido es necesario cuando el **tama침o del modelo** (billones de par치metros) o el **tama침o del lote (*batch size*)** son demasiado grandes para la memoria de un 칰nico acelerador (GPU/TPU).

## 1. Paralelismo de Datos (*Data Parallelism*)

El **Paralelismo de Datos** es la estrategia m치s com칰n y m치s f치cil de implementar. Se utiliza cuando el **modelo completo cabe en la memoria** de una sola GPU, pero el entrenamiento ser칤a demasiado lento o el *batch size* deseado es demasiado grande para procesarse de una vez.

### A. Mecanismo

1.  **Modelo Duplicado:** Se crea una **copia id칠ntica** del modelo ($\theta$) en cada uno de los $N$ dispositivos (GPUs/TPUs).
2.  **Datos Divididos:** El lote de datos global se divide en $N$ sub-lotes (*mini-batches*). Cada dispositivo procesa su propio sub-lote de forma independiente.
3.  **C치lculo:** Cada dispositivo calcula el pase hacia adelante y hacia atr치s, generando sus propios **gradientes locales** ($\nabla_i$).
4.  **Sincronizaci칩n:** Todos los gradientes locales se **agregan y promedian** entre todos los dispositivos (t칤picamente utilizando la t칠cnica **All-Reduce**).
5.  **Actualizaci칩n:** El modelo maestro o cada dispositivo individual actualiza sus pesos utilizando el gradiente promedio, garantizando que **todos los modelos permanezcan id칠nticos** al comienzo del siguiente paso.



### B. Ventajas y Limitaciones

* **Ventaja:** **F치cil de implementar** y proporciona una **aceleraci칩n lineal** casi perfecta con el n칰mero de dispositivos. El c칩digo base del modelo no necesita ser modificado.
* **Limitaci칩n:** **No resuelve el problema de la memoria del modelo.** Si el modelo por s칤 mismo es demasiado grande para caber en una sola GPU (ej. 70 mil millones de par치metros), esta t칠cnica no funciona.

---

## 2. Paralelismo de Modelos (*Model Parallelism*)

El **Paralelismo de Modelos** se utiliza cuando el **modelo en s칤 mismo es demasiado grande** para caber en la memoria de una sola GPU/TPU (el escenario cr칤tico para los Modelos de Lenguaje Grandes, LLMs).

El modelo se divide en partes y cada parte se coloca en un dispositivo diferente. Existen dos enfoques principales:

### A. Paralelismo de Pipeline (*Pipeline Parallelism*)

1.  **Divisi칩n Vertical (Capas):** El modelo se divide verticalmente a lo largo de sus capas. Las primeras $K$ capas van a la GPU 1, las siguientes $M$ capas van a la GPU 2, y as칤 sucesivamente.
2.  **Ejecuci칩n:** La GPU 1 calcula las primeras capas y pasa las activaciones a la GPU 2. La GPU 2 calcula su bloque y pasa las activaciones a la GPU 3. La ejecuci칩n se asemeja a una **l칤nea de ensamblaje**.
3.  **Eficiencia:** Para evitar que las GPUs permanezcan inactivas (burbujas en el *pipeline*), se utiliza la micro-divisi칩n de los lotes y la programaci칩n avanzada.

### B. Paralelismo de Tensor (*Tensor/Layer Parallelism*)

1.  **Divisi칩n Horizontal (Matrices):** Dentro de una sola capa del Transformador, las matrices de pesos (la matriz de Atenci칩n o la matriz *Feed-Forward*) se dividen horizontal o verticalmente entre los dispositivos.
2.  **Ejecuci칩n:** Cada GPU ejecuta solo una porci칩n de las operaciones de matriz. Requiere comunicaci칩n de alta velocidad entre GPUs para intercambiar resultados parciales (*All-Gather*, *Reduce-Scatter*) antes de que la capa pueda pasar a la siguiente etapa.
3.  **Uso:** Crucial para modelos gigantescos (cientos de miles de millones de par치metros) donde incluso una sola capa es demasiado grande.



### C. Ventajas y Limitaciones

* **Ventaja:** Permite entrenar **modelos de tama침o arbitrario** que no cabr칤an en una sola GPU.
* **Limitaci칩n:** **Alta complejidad** de implementaci칩n. Introduce **latencia de comunicaci칩n** significativa, ya que la activaci칩n de una capa debe esperar a que la capa anterior termine. Esto reduce la eficiencia en comparaci칩n con el Paralelismo de Datos.

---

## 3. Estrategias H칤bridas (La Soluci칩n Est치ndar)

En la pr치ctica, el entrenamiento de los LLMs modernos combina ambas estrategias:

1.  **Paralelismo de Modelo:** El modelo se divide entre los dispositivos necesarios para que quepa en la memoria (ej. utilizando Paralelismo de *Pipeline* y/o *Tensor*).
2.  **Paralelismo de Datos:** Se aplica el Paralelismo de Datos a trav칠s de los grupos de dispositivos que contienen r칠plicas del modelo dividido.

* *Ejemplo:* Un modelo se divide en 4 GPUs (Paralelismo de Modelo). Para acelerar el entrenamiento, esta configuraci칩n de 4 GPUs se replica 8 veces, sumando 32 GPUs en total (Paralelismo de Datos). Esto permite un *batch size* 8 veces m치s grande.

Esta combinaci칩n es lo que permite a las organizaciones entrenar modelos con miles de millones de par치metros en grandes cl칰steres de aceleradores.


---

Continua: [[7-1-2](https://github.com/webmastervetea/advanced-ai-curriculum/blob/main/temarios/7-1-2-entrenamiento-distribuido-modelos-grandes.md)] 
