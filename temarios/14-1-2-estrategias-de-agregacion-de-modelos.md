# 游뱋 Estrategias de Agregaci칩n de Modelos: El N칰cleo de Federated Averaging (FedAvg)

El **Aprendizaje Federado (*Federated Learning*, FL)** permite entrenar un modelo de *Machine Learning* utilizando datos descentralizados en m칰ltiples clientes (dispositivos m칩viles, hospitales, bancos) sin que los datos abandonen su ubicaci칩n original. Sin embargo, el 칠xito de este paradigma recae enteramente en la estrategia utilizada para **agregar** las actualizaciones de los modelos locales en un 칰nico modelo global.

El algoritmo m치s influyente y ampliamente adoptado para esta tarea es el **Promedio Federado (*Federated Averaging*, FedAvg)**.

## 1. El Rol de la Agregaci칩n

La agregaci칩n es el proceso mediante el cual un **Servidor Central** combina las actualizaciones de peso (gradientes o modelos locales) recibidas de una cohorte de clientes durante una ronda de entrenamiento, con el objetivo de producir un nuevo modelo global mejorado.

La agregaci칩n debe ser:
* **Eficiente:** Debe ser r치pido para no ralentizar el proceso iterativo.
* **Justa:** Debe reflejar la contribuci칩n de los diferentes clientes.
* **Robusta:** Debe funcionar a pesar de la heterogeneidad de los datos y del sistema.

---

## 2. Federated Averaging (FedAvg): El Est치ndar de Oro

**FedAvg** es la soluci칩n propuesta por Google que combina la paralelizaci칩n local del entrenamiento con una agregaci칩n ponderada en el servidor. Su objetivo es minimizar la p칠rdida de la funci칩n global $\mathcal{L}(\mathbf{W})$ promediando la p칠rdida de todos los clientes.

### A. El Ciclo Operacional de FedAvg

El entrenamiento de FedAvg se lleva a cabo en rondas comunicacionales ($t$):

1.  **Distribuci칩n ($t$):** El servidor selecciona un subconjunto de clientes $K_t$ (por razones de eficiencia y disponibilidad) y env칤a el modelo global actual $\mathbf{W}_t$.
2.  **C칩mputo Local:** Cada cliente $k \in K_t$ recibe $\mathbf{W}_t$ y realiza m칰ltiples pasos de **Descenso de Gradiente Estoc치stico (SGD)** localmente usando sus datos privados $\mathcal{D}_k$.
    * **Hiperpar치metros Locales Clave:**
        * $E$: N칰mero de 칠pocas locales que el cliente entrena.
        * $B$: Tama침o del lote (*batch size*).
    * El entrenamiento local produce un modelo actualizado $\mathbf{W}_{t+1}^k$.
3.  **Agregaci칩n Ponderada:** El servidor recibe los modelos actualizados $\mathbf{W}_{t+1}^k$ de los clientes. El nuevo modelo global $\mathbf{W}_{t+1}$ se calcula como el promedio ponderado de los modelos locales. La clave de FedAvg es que el peso de cada cliente es proporcional al **tama침o de su conjunto de datos local ($n_k$)**.

$$\mathbf{W}_{t+1} \leftarrow \sum_{k \in K_t} \frac{n_k}{N_t} \mathbf{W}_{t+1}^k$$

Donde $n_k$ es el n칰mero de muestras de datos del cliente $k$, y $N_t = \sum_{k \in K_t} n_k$ es el n칰mero total de muestras utilizadas en la ronda $t$. 

### B. Ventajas de FedAvg

* **Eficiencia Comunicacional:** Al realizar m칰ltiples 칠pocas locales ($E > 1$) en cada cliente, FedAvg **reduce la frecuencia de comunicaci칩n** con el servidor, lo cual es vital para dispositivos con conexiones lentas.
* **Simpleza:** El algoritmo de agregaci칩n es simple y f치cil de implementar.
* **Aprovechamiento de Recursos:** Permite que los clientes con mayor poder de c칩mputo y m치s datos contribuyan con un entrenamiento m치s extenso.

---

## 3. Desaf칤os y Extensiones de FedAvg

FedAvg, si bien es robusto, se enfrenta a desaf칤os importantes relacionados con la naturaleza descentralizada de los datos.

### A. Desaf칤o de Datos No-IID (Heterogeneidad)

El problema m치s grande de FedAvg ocurre cuando los datos de los clientes est치n **distribuidos de forma no independiente e id칠ntica (Non-IID)**. Si un cliente entrena 칰nicamente con datos de un dominio espec칤fico (ej. fotos de perros), su actualizaci칩n local puede "alejar" el modelo global de un buen desempe침o en otros dominios (ej. gatos).

* **Divergencia de Pesos:** El entrenamiento local prolongado ($E$ alto) en datos No-IID puede causar que los modelos locales diverjan significativamente antes de la agregaci칩n, lo que resulta en un modelo global sub칩ptimo o en una convergencia lenta.

### B. Extensiones para Mitigar la Divergencia

Para abordar el problema Non-IID, han surgido extensiones de FedAvg:

* **FedProx:** Modifica la funci칩n de p칠rdida local de cada cliente a침adiendo un **t칠rmino de regularizaci칩n de proximidad** $\mu$. Este t칠rmino penaliza las actualizaciones de peso que se alejan demasiado del modelo global anterior ($\mathbf{W}_t$). Esto evita que los clientes Non-IID se desv칤en demasiado.
* **SCAFFOLD:** Introduce un mecanismo para **corregir el sesgo** de los gradientes locales causados por la heterogeneidad de los datos. Cada cliente y el servidor mantienen variables de control para estimar y corregir la diferencia entre el gradiente local y el gradiente global ideal.

---

## 4. Agregaci칩n Robusta (Seguridad)

Otro desaf칤o crucial es la **robustez** ante clientes maliciosos o fallos de datos.

### A. Ataques de Envenenamiento de Datos (*Data Poisoning*)

Un cliente malicioso podr칤a enviar gradientes dise침ados espec칤ficamente para degradar el rendimiento del modelo global o para inyectar una "puerta trasera" (*backdoor*) en el modelo.

### B. T칠cnicas de Agregaci칩n Segura

* **Filtrado (*Trimming*):** Se eliminan las actualizaciones de clientes que son valores at칤picos (*outliers*) extremos, bas치ndose en la distancia de los gradientes al gradiente medio.
* **Mediana Ponderada (*Weighted Median*):** En lugar de promediar los pesos, se utiliza la mediana ponderada. La mediana es intr칤nsecamente m치s robusta a los valores at칤picos que el promedio.

En resumen, **FedAvg** proporcion칩 el *framework* inicial para la descentralizaci칩n del entrenamiento. Las estrategias de agregaci칩n actuales se centran en mejorar la robustez de FedAvg ante la heterogeneidad y garantizar la seguridad del modelo resultante.
