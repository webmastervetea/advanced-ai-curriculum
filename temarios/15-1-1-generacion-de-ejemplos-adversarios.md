# 游놑 Ejemplos Adversarios: La Vulnerabilidad Oculta de los Modelos de Clasificaci칩n

Los **Ejemplos Adversarios (*Adversarial Examples*)** son *inputs* dise침ados maliciosamente que se parecen casi id칠nticos a los datos leg칤timos para un observador humano, pero que provocan que un modelo de *Machine Learning* (especialmente las Redes Neuronales Profundas, DNNs) haga una **clasificaci칩n err칩nea y espec칤fica** con alta confianza.

El estudio de estos ataques es fundamental para la **Seguridad de la Inteligencia Artificial (AI Security)**, ya que revelan vulnerabilidades fundamentales en el aprendizaje profundo y tienen implicaciones cr칤ticas en sistemas de alto riesgo (como reconocimiento facial, detecci칩n de malware y veh칤culos aut칩nomos).

## 1. El Fen칩meno Adversario

El ataque funciona al explotar la **linealidad** y la **alta dimensi칩n** de las redes neuronales.

* **Mecanismo:** Un peque침o, pero intencional, **ruido imperceptible** se a침ade al *input* original ($\mathbf{x}$). Este ruido ($\mathbf{\eta}$) est치 dise침ado para explotar los l칤mites de decisi칩n del modelo.
* **Resultado:** El nuevo *input* perturbado ($\mathbf{x}_{\text{adv}} = \mathbf{x} + \mathbf{\eta}$) se clasifica err칩neamente.
* **Ejemplo Cl치sico:** Una imagen de un **Panda** se altera con un ruido min칰sculo, lo que hace que la red la clasifique con alta confianza como un **Gib칩n**. 

---

## 2. Ataques White-Box (Caja Blanca)

En los ataques **White-Box**, el atacante tiene un **conocimiento completo** sobre el modelo objetivo: su arquitectura, sus par치metros (pesos y sesgos) y los gradientes del modelo. Este conocimiento permite al atacante calcular el vector de ruido ($\mathbf{\eta}$) de forma extremadamente precisa.

### A. M칠todo de la Se침al de Gradiente R치pida (FGSM)

El **Fast Gradient Sign Method (FGSM)** es uno de los ataques *White-Box* m치s antiguos y eficientes.

* **Mecanismo:** El atacante calcula el **gradiente** de la funci칩n de p칠rdida del modelo con respecto a la entrada de la imagen original. Este gradiente indica en qu칠 direcci칩n el *input* debe moverse para maximizar la p칠rdida (es decir, maximizar la probabilidad de que el modelo se equivoque).
* **F칩rmula:** La perturbaci칩n $\mathbf{\eta}$ se calcula como:
$$\mathbf{\eta} = \epsilon \cdot \text{sign}(\nabla_{\mathbf{x}} J(\mathbf{\theta}, \mathbf{x}, y))$$
Donde $J$ es la funci칩n de p칠rdida, $\nabla_{\mathbf{x}}$ es el gradiente con respecto a la entrada $\mathbf{x}$, $\epsilon$ es la magnitud de la perturbaci칩n (controla qu칠 tan invisible es el ruido) y $\text{sign}$ toma el signo del gradiente.
* **Ventaja:** Es muy r치pido, ya que solo requiere un paso de *backpropagation*.

### B. Ataque Iterativo (PGD)

El ataque **Projected Gradient Descent (PGD)** es una versi칩n m치s fuerte de FGSM que utiliza m칰ltiples iteraciones y una proyecci칩n para refinar el ruido. Se considera el punto de referencia de los ataques *White-Box* en la investigaci칩n.

---

## 3. Ataques Black-Box (Caja Negra)

En los ataques **Black-Box**, el atacante **no tiene acceso** a los pesos, la arquitectura o los gradientes internos del modelo. El atacante solo puede interactuar con el modelo a trav칠s de la interfaz p칰blica, enviando *inputs* y recibiendo *outputs* (predicciones).

Estos ataques son mucho m치s realistas en el mundo real.

### A. Ataques Basados en Transferibilidad

Los ataques *Black-Box* explotan una propiedad clave de los modelos adversarios: la **transferibilidad**.

* **Mecanismo:** El atacante entrena un **Modelo Sustituto (*Surrogate Model*)** que intenta imitar el comportamiento del modelo objetivo *Black-Box*. Luego, el atacante genera Ejemplos Adversarios *White-Box* usando el Modelo Sustituto (ej. usando FGSM).
* **Transferencia:** El atacante observa que, sorprendentemente, los Ejemplos Adversarios creados para el Modelo Sustituto a menudo son efectivos para enga침ar al modelo objetivo *Black-Box*. Esto sucede porque las redes neuronales profundas tienden a aprender funciones de decisi칩n con l칤mites de decisi칩n similares.

### B. Ataques Basados en la Consulta (*Query-Based*)

Estos ataques intentan estimar el gradiente o la informaci칩n relevante del modelo *Black-Box* mediante la realizaci칩n de m칰ltiples consultas.

* **Mecanismo:** El atacante realiza muchas peque침as perturbaciones aleatorias en la imagen de entrada y observa c칩mo cambia la salida del modelo. Esto le permite estimar el gradiente del modelo **sin acceso directo** a su estructura interna.
* **Costo:** Estos ataques son muy efectivos, pero son costosos en t칠rminos de **n칰mero de consultas** que deben hacerse.

---

## 4. Implicaciones y Defensas

### A. Implicaciones Cr칤ticas

* **Seguridad:** Un atacante puede crear una pegatina invisible para una se침al de tr치fico que haga que un coche aut칩nomo la clasifique como "L칤mite de Velocidad 100", lo que demuestra el peligro en la toma de decisiones cr칤ticas.
* **Inyecci칩n de C칩digo:** Los Ejemplos Adversarios se han utilizado para modificar *inputs* de texto de manera imperceptible para evitar filtros de contenido o inyectar comandos maliciosos.

### B. Defensas (Entrenamiento Adversario)

La principal defensa contra estos ataques es el **Entrenamiento Adversario (*Adversarial Training*)**.

* **Mecanismo:** El modelo se entrena iterativamente con sus propios **Ejemplos Adversarios generados en cada 칠poca**. El objetivo es forzar al modelo a clasificar correctamente no solo los datos originales, sino tambi칠n sus versiones perturbadas.
* **Resultado:** Esto suaviza los l칤mites de decisi칩n del modelo en el vecindario del dato, haciendo que la red sea m치s robusta y menos sensible a las peque침as perturbaciones.

La lucha entre los ataques adversarios y las defensas es un campo activo y esencial que impulsa la investigaci칩n hacia modelos de IA m치s robustos y confiables.
