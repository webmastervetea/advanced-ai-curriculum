#  Atenci贸n Bidireccional vs. Atenci贸n Enmascarada (Unidireccional)

La distinci贸n clave entre las arquitecturas de Codificador (*Encoder-only*, como BERT) y las de Decodificador (*Decoder-only*, como GPT) reside en la forma en que el mecanismo de **Auto-Atenci贸n (*Self-Attention*)** accede al contexto de la secuencia.

## 1. Atenci贸n Bidireccional (BERT)

La Atenci贸n Bidireccional se utiliza en los modelos de tipo **Codificador** que est谩n dise帽ados para la **comprensi贸n** profunda del lenguaje.

### Mecanismo

Cuando BERT calcula el *embedding* contextual para un *token* en la posici贸n $i$ de una frase, el mecanismo de Auto-Atenci贸n tiene acceso completo a **todos los dem谩s *tokens*** en la secuencia, tanto los que est谩n a su **izquierda** (los que preceden) como los que est谩n a su **derecha** (los que siguen).

* **Acceso al Contexto:** El *token* $i$ puede "atender" y extraer informaci贸n de los *tokens* $1, 2, \dots, i-1, i+1, \dots, N$ (donde $N$ es la longitud total de la secuencia).
* **Finalidad:** Esto permite que el modelo genere una representaci贸n vectorial para cada palabra que est谩 informada por el contexto m谩s rico y completo posible. Esta representaci贸n es ideal para tareas donde se necesita un entendimiento integral de la frase (ej. clasificar el sentimiento de una rese帽a completa).

### La Importancia del MLM

El entrenamiento de BERT con el objetivo **MLM (*Masked Language Model*)** refuerza esta bidireccionalidad. Al enmascarar una palabra y pedirle al modelo que la prediga, el modelo se ve obligado a utilizar el contexto de ambos lados para adivinar con precisi贸n.

* **Ejemplo:** En la frase: "El **[MASK]** estaba caliente", BERT puede usar tanto "El" (izquierda) como "estaba caliente" (derecha) para inferir que la palabra enmascarada es probablemente un sustantivo singular que se puede calentar (ej. *sol*, *caf茅* o *horno*).

## 2. Atenci贸n Enmascarada / Causal (GPT)

La Atenci贸n Enmascarada, tambi茅n conocida como **Atenci贸n Causal**, se utiliza en los modelos de tipo **Decodificador** que est谩n dise帽ados para la **generaci贸n secuencial** (auto-regresiva).

### Mecanismo

La atenci贸n causal impone una restricci贸n artificial al mecanismo de Auto-Atenci贸n:

* **Enmascaramiento:** Cuando GPT genera el *token* en la posici贸n $i$, la matriz de atenci贸n aplica una **m谩scara triangular superior** a la matriz de puntuaciones de atenci贸n. Esta m谩scara establece efectivamente las puntuaciones de atenci贸n en las posiciones $i+1, i+2, \dots, N$ (el contexto futuro) a cero o un valor negativo muy grande.
* **Acceso al Contexto:** El *token* $i$ solo puede atender a los *tokens* $1, 2, \dots, i-1$ (el contexto que lo precede). El modelo no tiene acceso a lo que va a generar a continuaci贸n.

* **Finalidad:** Esta restricci贸n **imita el flujo natural del lenguaje humano** (hablamos o escribimos secuencialmente, sin saber la siguiente palabra). Es fundamental para garantizar que el modelo aprenda a generar texto de forma coherente. Si el modelo pudiera ver la respuesta completa, no aprender铆a a predecir el siguiente *token* bas谩ndose 煤nicamente en el historial.



## 3. Resumen de la Diferencia

| Caracter铆stica | Atenci贸n Bidireccional (BERT) | Atenci贸n Enmascarada (GPT) |
| :--- | :--- | :--- |
| **Arquitectura Principal** | Codificador (*Encoder*) | Decodificador (*Decoder*) |
| **Direccionalidad** | Bidireccional (Izquierda y Derecha) | Unidireccional / Causal (Solo Izquierda) |
| **Restricci贸n** | No hay. Acceso libre a toda la secuencia. | S铆. M谩scara triangular superior para bloquear el contexto futuro. |
| **Funci贸n Primaria** | **Comprensi贸n** y extracci贸n de caracter铆sticas. | **Generaci贸n** secuencial y auto-regresiva. |
| **Ejemplo de Tarea** | Clasificaci贸n de texto. | Respuesta a *prompts*, generaci贸n de historias. |


---

Continua: [[5-3]()] 
