

## 游뱋 Interacci칩n Humano-Robot (HRI) Segura: Pol칤ticas de Predicci칩n y Proactividad

La HRI segura se define como la capacidad del robot para coexistir en el mismo espacio de trabajo que un humano sin causar da침o, garantizando al mismo tiempo la continuidad del trabajo. Esto requiere que el robot sea **perceptivo** (prediciendo la intenci칩n humana) y **proactivo** (adaptando su movimiento).

### 1. El Paradigma de la Seguridad en HRI

La seguridad en HRI se rige por normas como la **ISO 10218** y la **ISO/TS 15066**, que definen cuatro estrategias principales de seguridad, donde la IA juega un papel crucial:

1.  **Parada de Seguridad Supervisada (*Safety-rated monitored stop*):** El robot se detiene instant치neamente cuando el humano entra en la zona de trabajo. (Reactivo, pero seguro).
2.  **Control de Velocidad y Separaci칩n:** El robot reduce su velocidad proporcionalmente a la distancia del humano. (Proactivo, basado en la proximidad).
3.  **Potencia y Fuerza Limitadas:** El robot tiene l칤mites estrictos de fuerza y energ칤a cin칠tica, minimizando el da침o en caso de impacto. (Dise침o f칤sico y control de bajo nivel).
4.  **Gu칤a Manual:** El humano mueve el robot directamente.

El objetivo de la IA es permitir la estrategia m치s flexible: el **Control de Velocidad y Separaci칩n** basado en la **predicci칩n de intenci칩n**.

### 2. Predicci칩n de Intenciones Humanas: El Pilar de la Proactividad

Para ser proactivo, el robot debe predecir la trayectoria futura del humano y, lo que es m치s importante, el objetivo de su acci칩n.

#### A. Modelado de Secuencias (RNNs y *Transformers*)

La predicci칩n de movimiento se modela como una tarea de **Percepci칩n 4D (Espacio-Tiempo)**.

* **Entrada:** Una secuencia de datos espaciales (posici칩n $x, y, z$) y temporales ($t$) del humano, obtenidos mediante visi칩n (c치maras) o LiDAR.
* **Modelo:** Se utiliza una **Red Neuronal Recurrente (RNN) o un *Transformer*** para procesar la secuencia de movimiento reciente.
* **Funci칩n:** La red aprende la funci칩n $\hat{P}(S_{t+\Delta t} \mid S_{t-\tau:t})$, prediciendo la distribuci칩n de probabilidad de la pose futura del humano ($S_{t+\Delta t}$) en funci칩n del historial reciente.
* **Intenci칩n:** Los modelos avanzados predicen no solo la pose, sino tambi칠n el **objetivo final** (ej. "el humano va a recoger la pieza A" vs. "el humano va a limpiar el 치rea B"). Esto permite al robot adaptar su plan completo.

#### B. Modelos de Inferencia de Objetivos (*Goal Inference*)
Estos modelos van m치s all치 de la cinem치tica para inferir el **principio de optimalidad** subyacente de la acci칩n humana (asumiendo que el humano est치 actuando racionalmente para lograr una meta).

* **Mecanismo:** El robot utiliza un *framework* de **Teor칤a de Juegos** o **Aprendizaje por Refuerzo Inverso (IRL)** para inferir la funci칩n de utilidad (recompensa) que est치 optimizando el humano.
* **Resultado:** Si el robot infiere que la intenci칩n del humano es alcanzar un punto espec칤fico, puede **adaptar su propia trayectoria** para evitar esa regi칩n, incluso antes de que el humano comience a moverse de forma decisiva.

### 3. Pol칤ticas de Control Seguras Basadas en la Predicci칩n

Una vez que el robot tiene una predicci칩n probabil칤stica de la intenci칩n humana, utiliza una pol칤tica para garantizar la seguridad.

#### A. Campo de Potencial Din치mico (Dynamic Potential Fields)
Es un m칠todo de control que trata los obst치culos y los humanos como **campos repulsivos**.

* **Mecanismo:** El humano predicho se modela como un campo de potencial repulsivo que se **mueve en el tiempo** de acuerdo con la predicci칩n de la IA.
* **Seguridad Proactiva:** Si la probabilidad de que el humano se mueva a una posici칩n $X$ es alta, el campo repulsivo en $X$ se vuelve m치s fuerte, forzando al robot a planificar un camino que evite $X$ con un margen de seguridad.

#### B. Planificaci칩n Basada en la Certificaci칩n de Seguridad (Safety Certificates)
En lugar de depender de heur칤sticas, este enfoque utiliza matem치ticas formales.

* **Mecanismo:** Se utiliza un **Control de Funciones de Barrera (Control Barrier Functions, CBF)**. Las CBF son una restricci칩n matem치tica que garantiza que, para cualquier entrada de control (acci칩n del robot), el sistema nunca abandonar치 la regi칩n de seguridad (es decir, nunca colisionar치).
* **Funci칩n:** La pol칤tica de IA generada por RL o el planificador optimizado se **proyecta** sobre el conjunto de control permitido por la CBF, asegurando que la acci칩n m치s eficiente nunca viole la restricci칩n de seguridad formal.

### 4. Transparencia y Confort Humano (Psicolog칤a)

La HRI segura no es solo f칤sica, sino tambi칠n psicol칩gica.

* **Transparencia de Intenci칩n:** Los robots deben comunicar su propia intenci칩n y el porqu칠 de sus movimientos (ej. mediante luces, proyecciones l치ser o interfaces de usuario). Si un robot se detiene, debe comunicar "Me detengo porque has entrado en mi zona de trabajo".
* **Comodidad:** Un robot que se mueve de manera demasiado reactiva o impredecible puede causar estr칠s. Las pol칤ticas de IA deben buscar movimientos que no solo sean seguros, sino tambi칠n **suaves y predecibles** para el ojo humano.

---

### Conclusi칩n

La **Interacci칩n Humano-Robot Segura** es una tarea de **Inteligencia Artificial Multimodal y Predictiva**. Al integrar t칠cnicas de **Percepci칩n 4D** para la predicci칩n de intenciones y al utilizar pol칤ticas de control robustas como las **Funciones de Barrera (CBF)** y el **Modelado de Campos de Potencial Din치mico**, los robots pueden dejar de ser meramente reactivos para convertirse en colaboradores proactivos que navegan en entornos compartidos con eficiencia y garant칤a de seguridad f칤sica y psicol칩gica.
---

Continua: [[46.1](https://github.com/webmastervetea/advanced-ai-curriculum/blob/main/temarios/46-1.md)] 
