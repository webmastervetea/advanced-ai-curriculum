
## üè≠ AutoML: Automatizaci√≥n Completa del Flujo de Trabajo de Machine Learning

**AutoML** se refiere a la automatizaci√≥n de las decisiones m√°s cr√≠ticas y que consumen m√°s tiempo en la creaci√≥n de un sistema de *Machine Learning* funcional y de alto rendimiento. Esto incluye no solo la selecci√≥n de hiperpar√°metros, sino tambi√©n las etapas previas de preprocesamiento y la elecci√≥n de la arquitectura del modelo.

El objetivo es permitir que incluso usuarios con poca experiencia obtengan modelos de calidad sin la necesidad de un *expertise* manual en cada etapa del *pipeline*.

---

### 1. Componentes Clave del *Pipeline* de AutoML

El *pipeline* tradicional de ML se descompone en varios subproblemas que el AutoML busca resolver de manera automatizada:

| Componente del Pipeline | Tarea Manual | Tarea de AutoML |
| :--- | :--- | :--- |
| **1. Preprocesamiento** | Imputaci√≥n, codificaci√≥n categ√≥rica, escalado. | **Automated Feature Engineering** |
| **2. B√∫squeda de Arquitectura** | Elecci√≥n de la arquitectura (CNN, Bosque Aleatorio, etc.). | **Neural Architecture Search (NAS)** |
| **3. Optimizaci√≥n de Hiperpar√°metros** | Definir la tasa de aprendizaje, profundidad del √°rbol, etc. | **HPO (Hyperparameter Optimization)** |
| **4. Selecci√≥n de Algoritmos** | Probar diferentes modelos. | **Meta-Learning, B√∫squeda Conjunta** |
| **5. *Ensembling*** | Combinar salidas de varios modelos. | **Stacked Generalization** |

---

### 2. Automatizaci√≥n del Preprocesamiento y *Feature Engineering*

El primer paso para el AutoML es preparar los datos y generar nuevas caracter√≠sticas informativas.

#### A. Preprocesamiento Automatizado
El sistema debe decidir autom√°ticamente:

* **Imputaci√≥n:** ¬øQu√© m√©todo usar para los datos faltantes (media, mediana, un valor predictivo)?
* **Codificaci√≥n:** ¬øC√≥mo codificar variables categ√≥ricas (One-Hot, *Label Encoding*, *Target Encoding*)?
* **Escalado:** ¬øEs necesario escalar las caracter√≠sticas (Normalizaci√≥n, Estandarizaci√≥n) y qu√© m√©todo es mejor?

#### B. Ingenier√≠a de Caracter√≠sticas Automatizada (*AFE*)
Utiliza los m√©todos discutidos anteriormente (Algoritmos Gen√©ticos, *Featuretools* o Autoencoders) para generar nuevas caracter√≠sticas a partir de las originales, mejorando el poder predictivo de los datos.

### 3. B√∫squeda de Arquitectura y Algoritmos (CASH)

El problema de **CASH (Combined Algorithm Selection and Hyperparameter optimization)** busca simult√°neamente el mejor algoritmo y el mejor conjunto de hiperpar√°metros para ese algoritmo.

#### A. Espacio de B√∫squeda Jer√°rquico
El espacio de b√∫squeda se modela como un **grafo dirigido ac√≠clico (DAG)** o un √°rbol jer√°rquico:

1.  **Nivel Superior (Algoritmo):** Nodo ra√≠z que elige el tipo de modelo (XGBoost, SVC, ANN).
2.  **Nivel Inferior (Hiperpar√°metros):** Los nodos dependientes eligen los hiperpar√°metros espec√≠ficos de ese modelo (ej., para XGBoost: `max_depth`, `learning_rate`).

#### B. Optimizaci√≥n de Hiperpar√°metros (HPO)

Una vez que se selecciona una arquitectura o algoritmo, se debe encontrar el mejor conjunto de hiperpar√°metros.

* **Optimizaci√≥n Bayesiana (OB):** Es la t√©cnica dominante. Utiliza un **modelo sustituto** (Proceso Gaussiano) para predecir el rendimiento de configuraciones de hiperpar√°metros no probadas. Utiliza una **funci√≥n de adquisici√≥n** para elegir el siguiente conjunto de par√°metros a probar, siendo mucho m√°s eficiente que la B√∫squeda en Malla (*Grid Search*) o la B√∫squeda Aleatoria (*Random Search*).
* **T√©cnicas de Presupuesto (*Budgeting*) (Successive Halving, Hyperband):** Se utiliza para acelerar el proceso. Comienzan probando muchos modelos con recursos limitados (pocas √©pocas) y eliminan progresivamente los de bajo rendimiento, reservando los recursos completos para los candidatos m√°s prometedores.

### 4. B√∫squeda de Arquitectura Neuronal (NAS) en AutoML

Para *Deep Learning*, el AutoML integra NAS para dise√±ar redes completas:

* **NAS como HPO:** En el contexto de AutoML, NAS puede considerarse una HPO especializada, donde los "hiperpar√°metros" incluyen el n√∫mero de capas y los tipos de bloques.
* **SuperRedes (One-Shot NAS):** Los sistemas avanzados de AutoML utilizan una **SuperRed** que codifica todas las arquitecturas candidatas. Esto permite que el *pipeline* eval√∫e el rendimiento de muchas sub-arquitecturas sin entrenarlas individualmente, lo que reduce el costo de la NAS en √≥rdenes de magnitud.

### 5. Meta-Aprendizaje (*Meta-Learning*)

Para acelerar la selecci√≥n inicial de algoritmos, el AutoML utiliza **Meta-Learning** para aprovechar el conocimiento adquirido en *datasets* previos.

* **Mecanismo:** El sistema almacena un **meta-dataset** de *datasets* y sus rendimientos asociados con diferentes algoritmos.
* **Predicci√≥n:** Cuando se introduce un nuevo *dataset*, el sistema analiza sus **meta-caracter√≠sticas** (tama√±o, n√∫mero de caracter√≠sticas categ√≥ricas, linealidad) y utiliza el conocimiento pasado para predecir **qu√© algoritmo es el mejor punto de partida**.
* **Beneficio:** En lugar de empezar desde cero, el AutoML puede priorizar una peque√±a selecci√≥n de algoritmos prometedores (ej., "para este tipo de datos, los modelos basados en √°rboles casi siempre funcionan mejor que las SVM").

---

### Conclusi√≥n

El AutoML transforma el ciclo de vida del ML en un problema de optimizaci√≥n automatizada. Al integrar t√©cnicas avanzadas como la Optimizaci√≥n Bayesiana para HPO y los m√©todos de SuperRedes para NAS, el AutoML permite una exploraci√≥n eficiente y rigurosa del vasto espacio de *pipelines* de ML. Esto democratiza la creaci√≥n de modelos de alto rendimiento y libera a los cient√≠ficos de datos para centrarse en problemas de m√°s alto nivel como la formulaci√≥n del problema y la √©tica del modelo.
---

Continua: [[32-1-1](https://github.com/webmastervetea/advanced-ai-curriculum/blob/main/temarios/32-1-1.md)] 
