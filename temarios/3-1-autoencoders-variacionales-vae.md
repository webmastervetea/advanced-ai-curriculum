#  Autoencoders Variacionales (VAE): Aprendizaje de Representaciones y Generaci贸n Probabil铆stica

Los **Autoencoders Variacionales (VAE)** son una clase de modelos de aprendizaje profundo que combinan la estructura del **Autoencoder** tradicional con principios de la **Estad铆stica Bayesiana** y el **Aprendizaje Variacional**. Su objetivo principal no es solo la compresi贸n de datos y el aprendizaje de representaciones (como un autoencoder est谩ndar), sino tambi茅n la **generaci贸n de datos nuevos** mediante el aprendizaje de una distribuci贸n de probabilidad subyacente de los datos.

## 1. Contexto: De Autoencoder a VAE

### El Autoencoder Cl谩sico

Un **Autoencoder** (*AE*) se compone de dos partes:

1.  **Codificador (*Encoder*):** Mapea un dato de entrada $\mathbf{x}$ (ej. una imagen) a un vector de baja dimensi贸n $\mathbf{z}$ (la **representaci贸n latente** o **c贸digo**): $q(\mathbf{z}|\mathbf{x})$.
2.  **Decodificador (*Decoder*):** Reconstruye el dato de entrada $\mathbf{x}$ a partir del c贸digo latente $\mathbf{z}$: $p(\mathbf{x}|\mathbf{z})$.

El AE se entrena minimizando la **p茅rdida de reconstrucci贸n** (ej. Error Cuadr谩tico Medio) entre la entrada $\mathbf{x}$ y la salida reconstruida $\mathbf{x}'$. El problema es que el espacio latente $\mathbf{z}$ aprendido por un AE cl谩sico puede ser **irregular** y tener "huecos". Al muestrear un punto $\mathbf{z}$ al azar, el decodificador podr铆a generar una salida sin sentido.

### La Soluci贸n del VAE

El VAE resuelve este problema imponiendo una **estructura probabil铆stica** al espacio latente. Obliga al codificador a mapear los datos de entrada a distribuciones que se asemejen a una **distribuci贸n a priori simple** (t铆picamente una distribuci贸n Gaussiana est谩ndar o *normal* $\mathcal{N}(0, I)$). Esto asegura que el espacio latente sea **continuo** y **suave**, haciendo que el muestreo aleatorio sea coherente para la generaci贸n.



---

## 2. Arquitectura y Funcionamiento

A diferencia del AE, el **Codificador del VAE** no produce directamente el vector $\mathbf{z}$, sino los par谩metros de la distribuci贸n de probabilidad que gobierna la representaci贸n latente.

### A. El Codificador (Inferencia)

El codificador, $q(\mathbf{z}|\mathbf{x})$, toma la entrada $\mathbf{x}$ y genera dos vectores:

1.  **Vector de Media ($\mu$):** Controla la posici贸n central de la distribuci贸n latente.
2.  **Vector de Desviaci贸n Est谩ndar ($\sigma$, o log-varianza $\log\sigma^2$):** Controla la dispersi贸n o la "cantidad de ruido" permitida alrededor de la media.

El vector latente $\mathbf{z}$ se extrae (muestra) de esta distribuci贸n codificada:

$$\mathbf{z} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$$

### B. El Truco de la Reparametrizaci贸n

Para entrenar la red usando *backpropagation*, necesitamos que el gradiente fluya desde la p茅rdida de reconstrucci贸n de vuelta al codificador. Sin embargo, el **muestreo** es un proceso aleatorio no diferenciable.

El **Truco de la Reparametrizaci贸n** (*Reparameterization Trick*) resuelve esto: en lugar de muestrear $\mathbf{z}$ directamente, reescribimos el proceso de muestreo como una funci贸n determinista de dos componentes:

$$\mathbf{z} = \boldsymbol{\mu} + \boldsymbol{\sigma} * \boldsymbol{\epsilon}$$

Donde:
* $\boldsymbol{\epsilon}$ es un vector de ruido muestreado de una Gaussiana est谩ndar ($\mathcal{N}(0, I)$).
* $\boldsymbol{\sigma}$ es la desviaci贸n est谩ndar (calculada a partir de la log-varianza del codificador).
* $*$ es la multiplicaci贸n elemento a elemento.

Al separar la aleatoriedad ($\boldsymbol{\epsilon}$) del proceso de aprendizaje ($\boldsymbol{\mu}, \boldsymbol{\sigma}$), el gradiente ahora puede fluir a trav茅s de los par谩metros $\boldsymbol{\mu}$ y $\boldsymbol{\sigma}$ del codificador.

### C. El Decodificador (Generaci贸n)

El decodificador, $p(\mathbf{x}|\mathbf{z})$, es una red neuronal est谩ndar que toma el vector latente muestreado $\mathbf{z}$ y lo mapea de vuelta al espacio de datos, produciendo la reconstrucci贸n $\mathbf{x}'$.

---

## 3. La Funci贸n de P茅rdida del VAE (ELBO)

La funci贸n de p茅rdida de un VAE se conoce como el **L铆mite Inferior de la Evidencia (Evidence Lower Bound, ELBO)**. Minimizar la p茅rdida del VAE es equivalente a maximizar el ELBO. La p茅rdida se compone de dos t茅rminos principales:

$$\mathcal{L}(\theta, \phi) = \text{P茅rdida de Reconstrucci贸n} + \text{P茅rdida KL}$$

### A. P茅rdida de Reconstrucci贸n (Fidelidad)

Mide qu茅 tan bien el decodificador reconstruye la entrada original. Es la forma est谩ndar de p茅rdida del autoencoder, pero vista a trav茅s de una lente probabil铆stica (como la esperanza negativa del logaritmo de la verosimilitud).

$$\text{P茅rdida de Reconstrucci贸n} = - \mathbb{E}_{\mathbf{z} \sim q(\mathbf{z}|\mathbf{x})} [\log p(\mathbf{x}|\mathbf{z})]$$

* **Funci贸n:** Asegura que las salidas del decodificador sean similares a las entradas $\mathbf{x}$.

### B. P茅rdida de Divergencia KL (Regularizaci贸n)

Este t茅rmino es exclusivo del VAE. Utiliza la **Divergencia de Kullback-Leibler ($\text{KL}$)** para medir la diferencia entre la distribuci贸n latente aprendida por el codificador $q(\mathbf{z}|\mathbf{x})$ y la distribuci贸n *a priori* deseada $p(\mathbf{z})$ (la Gaussiana est谩ndar).

$$\text{P茅rdida KL} = D_{KL}(q(\mathbf{z}|\mathbf{x}) || p(\mathbf{z}))$$

* **Funci贸n:** Act煤a como un **regularizador**, obligando a las distribuciones latentes de las entradas a ser similares a la Gaussiana est谩ndar $\mathcal{N}(0, I)$. Esto asegura que el espacio latente sea **suave, continuo y denso** alrededor del origen, garantizando que el muestreo aleatorio en esta 谩rea genere datos coherentes.

**El VAE es un balance:** Intenta minimizar la P茅rdida de Reconstrucci贸n (ser preciso) y al mismo tiempo minimizar la P茅rdida KL (mantener el espacio latente ordenado y suave para la generaci贸n).

---

## 4. Generaci贸n de Nuevos Datos

Una vez que el VAE ha sido entrenado, se descarta el codificador. El decodificador se convierte en un **modelo generativo**.

El proceso de generaci贸n es simple:

1.  **Muestreo Latente:** Se muestrea un vector $\mathbf{z}$ al azar directamente de la distribuci贸n *a priori* (ej. $\mathbf{z} \sim \mathcal{N}(0, I)$).
2.  **Decodificaci贸n:** El vector $\mathbf{z}$ se alimenta al decodificador para producir un nuevo dato $\mathbf{x}_{\text{gen}}$.

Como el t茅rmino KL ha obligado a que todas las representaciones latentes caigan en una regi贸n suave y conectada, cualquier punto $\mathbf{z}$ muestreado cerca de esta regi贸n deber铆a ser decodificado en un dato v谩lido (aunque posiblemente novedoso) que se asemeje a los datos de entrenamiento.

## 5. Aplicaciones Clave

* **Generaci贸n de Im谩genes:** Los VAE fueron pioneros en la generaci贸n de im谩genes de rostros, objetos y ropa, aunque posteriormente fueron superados en calidad visual por los GANs y los Modelos de Difusi贸n.
* **Aprendizaje de Representaciones y Descomposici贸n:** Pueden aprender representaciones latentes que capturan factores explicativos de los datos (ej. un eje del espacio latente puede corresponder al color del cabello, otro al 谩ngulo de la cara).
* **Generaci贸n de Variantes y Manipulaci贸n de Atributos:** Permiten la manipulaci贸n sem谩ntica de los datos: movi茅ndose a lo largo de un "eje" latente espec铆fico, se puede cambiar un atributo (ej. hacer que una cara parezca m谩s sonriente) sin alterar significativamente otros atributos.
* **Detecci贸n de Anomal铆as:** Los VAE son excelentes para detectar valores at铆picos, ya que una entrada an贸mala tendr谩 una alta p茅rdida de reconstrucci贸n y una distribuci贸n latente muy diferente a la Gaussiana $\mathcal{N}(0, I)$.

Los VAE establecieron un puente crucial entre los m茅todos probabil铆sticos y las arquitecturas de redes neuronales, sentando las bases para gran parte del trabajo moderno en modelos generativos.




---

Continua: [[3-1-b1](https://github.com/webmastervetea/advanced-ai-curriculum/blob/main/temarios/3-1-b1-truco-reparametrizacion.md)] 
