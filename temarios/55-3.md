

## ⚖️ Detección y Mitigación de Sesgos en Datos Estructurados: IA Responsable

El sesgo en los datos estructurados (tablas, bases de datos) surge históricamente de prácticas discriminatorias, representaciones desiguales o sesgos de muestreo. Nuestro objetivo es que el modelo no aprenda ni perpetúe estas injusticias históricas.

### 1. Detección: Identificación y Medición del Sesgo

El primer paso es identificar las **variables protegidas** (ej. género, raza, edad) y determinar cómo están representadas en el *dataset* en relación con la variable objetivo.

#### A. Métricas de Desequilibrio (*Disparity Metrics*)

Estas métricas cuantifican la diferencia en la distribución entre los grupos.

1.  **Paridad Demográfica (*Demographic Parity*):** Mide si la **tasa de resultados positivos** es igual para todos los grupos.
    $$\text{Tasa positiva} = P(\text{Resultado} = 1 \mid \text{Grupo})$$
    * **Sesgo:** Si $P(\text{Resultado}=1 \mid \text{Grupo A}) \neq P(\text{Resultado}=1 \mid \text{Grupo B})$, existe sesgo de resultado.
2.  **Paridad de Impacto (*Disparate Impact*):** Mide la proporción de tasas positivas entre el grupo privilegiado y el no privilegiado.
    $$\text{Impacto Dispar} = \frac{\text{Tasa positiva Grupo No-Privilegiado}}{\text{Tasa positiva Grupo Privilegiado}}$$
    * **Umbral:** Un valor fuera del rango $[0.8, 1.25]$ a menudo se considera evidencia de impacto dispar, según las directrices del EEOC de EE. UU.

#### B. Detección de Sesgo de Representación
Se utilizan técnicas de Aprendizaje No Supervisado para detectar subgrupos subrepresentados.

* **Mecanismo:** Se aplica un algoritmo de **Agrupamiento (*Clustering*)** (ej. k-means) a las *features* del *dataset* para identificar grupos naturales de datos. Si un grupo natural importante (ej. pacientes con una enfermedad rara) tiene una representación minúscula, existe un sesgo de muestreo que afectará la capacidad del modelo para generalizar en ese grupo.

### 2. Mitigación: Técnicas a Nivel de Datos (*Pre-processing*)

La forma más efectiva y transparente de mitigar el sesgo es corregirlo antes de que el modelo vea los datos.

#### A. Re-muestreo (*Resampling*)
Ajustar la representación de las instancias en los grupos para lograr la paridad.

* **Sobremuestreo (*Oversampling*):** Duplicar o generar nuevas instancias sintéticas para el grupo subrepresentado.
* **Submuestreo (*Undersampling*):** Eliminar instancias del grupo sobrerrepresentado.



#### B. Re-etiquetado (*Reweighing*)
Cambiar el peso de las instancias en la función de pérdida para equilibrar la contribución de los grupos.

* **Mecanismo:** A cada punto de dato se le asigna un peso. Los puntos de dato del grupo subrepresentado reciben un peso mayor para que el modelo preste más atención a su predicción y las tasas de error sean compensadas.

#### C. Supresión de *Features*
Eliminar *features* que son proxies del grupo protegido (variables indirectas pero altamente correlacionadas).

* **Desafío:** Es difícil identificar todos los *proxies*. Por ejemplo, el código postal puede ser un *proxy* fuerte para la raza o la clase socioeconómica. Eliminar solo la variable protegida original (*gender* o *race*) no suele ser suficiente.

### 3. Mitigación: Técnicas Basadas en la Transformación

Estos métodos transforman las *features* del *dataset* para hacerlas "justas" antes de alimentar al modelo.

#### A. Aprendizaje de Representaciones Justas (*Adversarial Debiasing*)
Utilizar redes neuronales para aprender una representación de datos que no contenga información de sesgo.

* **Mecanismo:** Se entrena un **Generador** para crear *embeddings* de datos. Simultáneamente, se entrena un **Adversario (Discriminador)** cuya única tarea es predecir la variable protegida (ej. la raza) a partir del *embedding* del Generador.
* **Función:** El Generador es penalizado si el Adversario tiene éxito. Esto obliga al Generador a crear *embeddings* de datos que son **funcionalmente justos** (es decir, el *embedding* contiene información suficiente para la tarea principal, pero no contiene información discernible sobre la variable protegida).

### 4. Conclusión

La **Detección y Mitigación de Sesgos en Datos Estructurados** es una obligación ética y técnica. Se comienza por **cuantificar la disparidad** (ej. Paridad Demográfica) y luego se aplican técnicas de *pre-processing* como el **re-muestreo y re-etiquetado**. Para lograr la justicia técnica profunda, los métodos basados en la **Transformación Adversarial** aprenden una representación del dato que es intencionalmente ciega a las variables protegidas, garantizando que el sesgo no se transfiera al modelo final.

---

Continua: [[56.1](https://github.com/webmastervetea/advanced-ai-curriculum/blob/main/temarios/56-1.md)] 
