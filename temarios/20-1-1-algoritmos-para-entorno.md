# 游뱄 DDPG y TD3: Aprendizaje por Refuerzo en Espacios Continuos

El **Aprendizaje por Refuerzo Profundo (*Deep Reinforcement Learning*, DRL)** ha logrado un 칠xito notable en dominios complejos. Sin embargo, los primeros algoritmos de DRL se dise침aron principalmente para espacios de acci칩n discretos (ej. mover ficha, presionar un bot칩n). Para problemas del mundo real, como el control rob칩tico o la conducci칩n aut칩noma, las acciones (ej. 치ngulo del volante, torque del motor) son **continuas e infinitas**.

Los algoritmos **DDPG (*Deep Deterministic Policy Gradient*)** y **TD3 (*Twin Delayed Deep Deterministic Policy Gradient*)** son extensiones del DRL que resuelven este desaf칤o al combinar el **Aprendizaje por Refuerzo Basado en el Valor** (Q-Learning) con el **Aprendizaje Basado en la Pol칤tica** (Policy Gradients).

## 1. DDPG: La Fusi칩n Determinista para Espacios Continuos

**DDPG** fue uno de los primeros y m치s exitosos algoritmos para manejar espacios de acci칩n continuos. Se basa en el algoritmo **AC (*Actor-Critic*)**, pero utiliza el concepto de pol칤tica **determinista** y redes neuronales profundas.

### A. Arquitectura Actor-Cr칤tico

DDPG utiliza dos redes neuronales principales que trabajan conjuntamente (arquitectura *Actor-Critic*):

1.  **Actor ($\mu$):** Es la red de **pol칤tica**. Toma el **estado continuo** ($s$) como entrada y produce la **acci칩n continua** ($a$) directamente. Es **determinista** porque no produce una distribuci칩n de probabilidades, sino un valor de acci칩n espec칤fico: $a = \mu(s)$.
2.  **Cr칤tico ($Q$):** Es la red de **valor**. Toma el **estado ($s$) y la acci칩n ($a$)** como entrada y estima la **funci칩n de valor-acci칩n $Q(s, a)$**. El Cr칤tico ense침a al Actor indic치ndole qu칠 tan buena es la acci칩n que eligi칩.

### B. Redes *Target* y *Experience Replay*

Para lograr estabilidad en el entrenamiento de las redes profundas, DDPG introduce t칠cnicas del algoritmo DQN:

* **Redes *Target*:** Se mantienen copias congeladas (ligeramente retrasadas) de las redes Actor y Cr칤tico. Estas redes *target* se utilizan para calcular la recompensa futura (el objetivo *target* de Q-Learning), lo que estabiliza el proceso de ajuste.
* ***Experience Replay Buffer*:** Las transiciones de experiencia ($s, a, r, s'$) se almacenan y se muestrean aleatoriamente para el entrenamiento, rompiendo la correlaci칩n secuencial de los datos.

### C. Desaf칤o de DDPG: La Sobreestimaci칩n

El problema principal de DDPG es que la red Cr칤tico ($Q$) tiende a **sobreestimar sistem치ticamente** los valores $Q$. Esta sobreestimaci칩n, al retroalimentarse al Actor, puede conducir a pol칤ticas sub칩ptimas y a una convergencia fr치gil.

---

## 2. TD3: Estabilizaci칩n a Trav칠s del Retraso y la Duplicidad

**TD3 (*Twin Delayed Deep Deterministic Policy Gradient*)** es una mejora directa y significativa sobre DDPG, dise침ada espec칤ficamente para contrarrestar la sobreestimaci칩n del valor y estabilizar la pol칤tica.

### A. El Triple Retraso (TD3)

TD3 introduce tres modificaciones clave para mejorar la estabilidad y el rendimiento:

1.  **Redes Cr칤ticas Gemelas (*Twin Critics*):** En lugar de usar una sola red Cr칤tico, TD3 utiliza **dos redes Cr칤ticas independientes** ($Q_1$ y $Q_2$). Para calcular el valor *target* de la recompensa futura, utiliza el **m칤nimo** de las dos estimaciones de $Q$:
    $$Q_{\text{target}} = r + \gamma \cdot \min(Q_1'(s', a'), Q_2'(s', a'))$$
    Esto reduce la sobreestimaci칩n ya que es menos probable que ambas redes Cr칤ticas sobreestimen la misma acci칩n.

2.  **Actualizaci칩n Retrasada del Actor (*Delayed Policy Updates*):** La red Actor se actualiza con menos frecuencia que las redes Cr칤ticas (ej. el Cr칤tico se actualiza 2 veces por cada 1 vez del Actor).
    * **Raz칩n:** Esto da tiempo a las redes Cr칤ticas para alcanzar una estimaci칩n de valor m치s precisa antes de que el Actor use esa informaci칩n para mejorar su pol칤tica.

3.  **Suavizado de la Pol칤tica (*Target Policy Smoothing*):** Se a침ade un **ruido aleatorio truncado** a la acci칩n *target* utilizada para calcular el *target* $Q$.
    * **Raz칩n:** Esto suaviza la funci칩n de valor sobre acciones similares, haciendo que el Cr칤tico sea menos susceptible a los picos de valor causados por las acciones del Actor.



## 3. Comparaci칩n y Aplicaciones

| Caracter칤stica | DDPG | TD3 |
| :--- | :--- | :--- |
| **Arquitectura** | Actor y Cr칤tico 칰nicos | Actor y **dos Cr칤ticos** gemelos |
| **Estabilidad** | Moderada; propenso a la sobreestimaci칩n | Alta; utiliza $\min(Q_1, Q_2)$ para prevenir la sobreestimaci칩n |
| **Actualizaci칩n del Actor**| Cada paso de entrenamiento | **Retrasada** (menos frecuente que el Cr칤tico) |
| **Suavizado** | No | S칤, a침ade ruido truncado a las acciones *target* |
| **Complejidad** | M치s simple | M치s complejo computacionalmente |

**Aplicaciones:** Ambos algoritmos son fundamentales para:

* **Rob칩tica de Alto Grado de Libertad:** Control de brazos rob칩ticos y manipuladores donde las articulaciones requieren 치ngulos y torques continuos.
* **Veh칤culos Aut칩nomos:** Control de aceleraci칩n, frenado y direcci칩n.
* **Control de Sistemas:** Regulaci칩n de variables continuas en plantas industriales o simulaciones f칤sicas.

En resumen, TD3 representa una mejora s칩lida sobre DDPG, ofreciendo una soluci칩n m치s estable y robusta al desaf칤o de entrenar pol칤ticas de acci칩n continua en el Aprendizaje por Refuerzo Profundo.
