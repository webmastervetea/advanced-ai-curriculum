
## 융 Fusión de Sensores de Bajo Nivel: Integración Temprana para la Percepción Robusta

La **Fusión de Sensores** se clasifica por la etapa del procesamiento donde ocurre la combinación:

1.  **Fusión Tarda (*Late Fusion* o *Decision-Level*):** Modelos separados procesan cada sensor y solo las predicciones finales (cajas delimitadoras, clasificaciones) se combinan. Simple, pero ignora las interacciones tempranas.
2.  **Fusión de Bajo Nivel (*Early Fusion* o *Feature-Level*):** Los datos brutos (o características de muy bajo nivel) se combinan inmediatamente para que un solo modelo de *Deep Learning* aprenda las correlaciones íntimas. Este es el tema de este análisis.

### 1. El Desafío de la Heterogeneidad de Sensores

La principal dificultad de la Fusión de Bajo Nivel es que los datos de los sensores son inherentemente heterogéneos:

* **Cámara:** Densidad de datos alta (millones de píxeles), información semántica rica (color, textura) pero es **2D** y pasiva (dependiente de la luz).
* **LiDAR:** Dispersión de datos baja (puntos escasos), información geométrica precisa (distancia, forma **3D**), pero sin textura ni color.
* **Radar:** Información de velocidad (Doppler) y alcance, inmune al clima, pero de resolución muy baja.

La red neuronal debe aprender a combinar estos formatos dispares manteniendo la coherencia espacial y temporal.

### 2. Mapeo y Sincronización Espacial (Geométrica)

Para fusionar datos crudos, primero deben estar en el mismo sistema de coordenadas y perfectamente sincronizados.

#### A. Proyección de Puntos LiDAR en la Imagen
El primer paso es proyectar los puntos LiDAR (coordenadas $x, y, z$) en el plano de la imagen de la cámara.

* **Mecanismo:** Se utiliza la **matriz de calibración intrínseca y extrínseca** del vehículo, que transforma el punto 3D del LiDAR en coordenadas de píxel 2D.
* **Resultado:** Se crea un canal adicional en la imagen (además de RGB) donde cada píxel tiene ahora información de **profundidad** y **reflexión** (intensidad) proveniente del LiDAR. Esto es la base del **enriquecimiento de características**.

#### B. Sincronización Temporal
Los sensores operan a diferentes frecuencias (ej. LiDAR a 10 Hz, cámara a 30 Hz). La fusión requiere interpolar o alinear las lecturas para que el *timestamp* coincida exactamente en el momento de la inferencia.

### 3. Técnicas de Fusión de Bajo Nivel (*Deep Learning*)

Las técnicas de *Deep Learning* utilizan el mapeo espacial para integrar las características desde las primeras capas de la red.

#### A. Fusión Temprana de Canales (Channel-Wise Early Fusion)


* **Mecanismo:** Después de la proyección, las salidas de los sensores se apilan como canales de entrada de un tensor.
    $$\text{Input Tensor} = [\text{Canal R, Canal G, Canal B, Canal Profundidad, Canal Intensidad}]$$
* **Procesamiento:** Una red neuronal convolucional (CNN) procesa directamente este tensor de múltiples canales.
* **Ventaja:** Permite que los filtros convolucionales aprendan a correlacionar las características de bajo nivel (ej. un borde en RGB con un cambio abrupto de profundidad en LiDAR) desde la primera capa.
* **Limitación:** Si las modalidades no están perfectamente alineadas o si una modalidad es de muy baja resolución, puede contaminar las características de la otra.

#### B. Fusión de Características Heterogéneas
En lugar de fusionar el *input* directamente, se permite que cada modalidad pase por un módulo inicial que estandariza o procesa ligeramente las características.

* **Mecanismo:**
    1.  **Red $R_V$ (Visión):** Genera características visuales $\mathbf{f}_V$ (ej. de la tercera capa de una CNN).
    2.  **Red $R_L$ (LiDAR):** Genera características geométricas $\mathbf{f}_L$ (ej. de una PointNet o VoxelNet).
    3.  **Módulo de Fusión:** Se combinan $\mathbf{f}_V$ y $\mathbf{f}_L$ (mediante concatenación o **atención cruzada**), y luego se pasan a un decodificador común.
* **Ventaja:** Permite que las redes $R_V$ y $R_L$ utilicen arquitecturas optimizadas para sus datos (ej. convoluciones 2D vs. 3D).

### 4. Fusión con Atención Cruzada (*Cross-Attention Fusion*)

Esta es la técnica más avanzada, ya que permite que los *embeddings* de un sensor guíen la extracción de características del otro.

* **Mecanismo:** Similar a la Fusión de Características vista en GCM, pero aplicada aquí a las características espaciales.
    1.  Se usa una característica (ej. $\mathbf{f}_V$ de la cámara) como **Consulta** ($\mathbf{Q}$).
    2.  La característica del LiDAR ($\mathbf{f}_L$) actúa como **Clave/Valor** ($\mathbf{K}/\mathbf{V}$).
    3.  El módulo de atención calcula qué puntos de $\mathbf{f}_L$ son más relevantes para cada elemento de $\mathbf{f}_V$.
* **Resultado:** Se genera una **característica visual enriquecida** que solo presta atención a los elementos geométricos (LiDAR) que son coherentes con lo que ve la cámara, logrando una fusión adaptativa y contextual.

### 5. Conclusión

La **Fusión de Sensores de Bajo Nivel** es crucial para la percepción robusta en sistemas de misión crítica. Si bien es geométricamente compleja, el uso de **Redes Neuronales Convolucionales** en canales fusionados o módulos de **Atención Cruzada** en *embeddings* tempranos permite que el modelo aprenda correlaciones profundas (ej. la sombra en el cielo es un falso positivo, ya que el LiDAR no detecta nada ahí), lo que resulta en una identificación y un mapeo del entorno mucho más fiables que cualquier sistema basado en la fusión tardía.

---

Continua: [[43.2](https://github.com/webmastervetea/advanced-ai-curriculum/blob/main/temarios/43-2.md)] 
