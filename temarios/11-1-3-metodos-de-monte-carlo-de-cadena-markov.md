#  MCMC y Variational Inference: La Aproximaci贸n de la Distribuci贸n Posterior

El objetivo central del **Aprendizaje Bayesiano** es calcular la **distribuci贸n posterior** $P(\mathbf{\theta}|\mathcal{D})$, que representa la probabilidad de los par谩metros del modelo ($\mathbf{\theta}$) dadas las observaciones ($\mathcal{D}$). Esta distribuci贸n es clave para la cuantificaci贸n de la incertidumbre y la toma de decisiones.

Sin embargo, para modelos complejos (como las redes neuronales profundas o modelos con muchas variables), calcular o muestrear directamente la distribuci贸n posterior es **anal铆ticamente intratable** e **intratable computacionalmente**. Aqu铆 es donde entran en juego los m茅todos de aproximaci贸n: **MCMC** y **Inferencia Variacional (VI)**.

## 1. M茅todos de Monte Carlo de Cadena de Markov (MCMC)

**MCMC** es una clase de algoritmos que generan una secuencia de muestras dependientes (una **cadena de Markov**) de una distribuci贸n objetivo. A medida que la cadena se ejecuta por un tiempo suficiente, la distribuci贸n emp铆rica de las muestras converge a la distribuci贸n posterior verdadera $P(\mathbf{\theta}|\mathcal{D})$.

### A. Mecanismo Fundamental: El Paseo Aleatorio

1.  **Cadena de Markov:** Es un proceso estoc谩stico donde la probabilidad de moverse a un nuevo estado ($\mathbf{\theta}_{t+1}$) depende 煤nicamente del estado actual ($\mathbf{\theta}_t$).
2.  **Muestreo:** El algoritmo est谩 dise帽ado para que la distribuci贸n de equilibrio (el estado estable) de la cadena de Markov sea exactamente la distribuci贸n posterior objetivo.
3.  **Algoritmo de Metropolis-Hastings (MH):** Es el MCMC m谩s famoso. En cada paso:
    * Se propone un nuevo estado candidato $\mathbf{\theta}'$.
    * Se acepta o rechaza $\mathbf{\theta}'$ bas谩ndose en la raz贸n de la densidad de probabilidad del estado candidato y el estado actual. Esto garantiza que la cadena se mueva hacia regiones de alta probabilidad.
4.  **Gibbs Sampling:** Un caso especial de MH utilizado cuando los par谩metros pueden agruparse y muestrearse individualmente de sus distribuciones condicionales (a menudo m谩s simples).

### B. Ventajas y Desventajas de MCMC

| Aspecto | Ventajas | Desventajas |
| :--- | :--- | :--- |
| **Precisi贸n** | Proporciona muestras que, en el l铆mite, son **exactas** (no sesgadas) de la distribuci贸n posterior verdadera. | **Lentitud y Escalabilidad:** Es muy lento para grandes conjuntos de datos y modelos con millones de par谩metros (ej. *Deep Learning*). |
| **Diagn贸stico** | Requiere un largo periodo de **quemado (*burn-in*)** para que la cadena converja a la distribuci贸n. | La convergencia es dif铆cil de diagnosticar formalmente. |
| **Robustez** | Es robusto a la forma compleja de la distribuci贸n posterior (ej. distribuciones multimodales). | **Correlaci贸n:** Las muestras consecutivas est谩n correlacionadas, lo que requiere m谩s muestras para lograr una precisi贸n equivalente a muestras independientes. |

---

## 2. Inferencia Variacional (VI)

La **Inferencia Variacional (VI)** reformula el problema de la inferencia Bayesiana como un **problema de optimizaci贸n**. En lugar de muestrear, VI busca encontrar la distribuci贸n m谩s cercana a la posterior verdadera dentro de una clase de distribuciones sencillas y manejables.

### A. Mecanismo Fundamental: Minimizaci贸n de la Distancia

1.  **Distribuci贸n Aproximada ($q$):** Se elige una distribuci贸n de aproximaci贸n simple, $q(\mathbf{\theta}|\mathbf{\lambda})$, gobernada por un conjunto de **par谩metros variacionales** ($\mathbf{\lambda}$). T铆picamente, $q$ es una distribuci贸n factorizada (ej. Gaussianas independientes) para simplificar los c谩lculos.
2.  **Distancia (Divergencia KL):** El objetivo es encontrar los par谩metros $\mathbf{\lambda}$ que minimicen la **Divergencia de Kullback-Leibler (KL)** entre $q$ y la posterior verdadera $P(\mathbf{\theta}|\mathcal{D})$:

$$\mathbf{\lambda}^* = \arg \min_{\mathbf{\lambda}} \text{KL}[q(\mathbf{\theta}|\mathbf{\lambda}) || P(\mathbf{\theta}|\mathcal{D})]$$

3.  **ELBO (Evidence Lower Bound):** Como la posterior $P(\mathbf{\theta}|\mathcal{D})$ es intratable, la minimizaci贸n de $\text{KL}[q || P]$ es matem谩ticamente equivalente a **maximizar el L铆mite Inferior de la Evidencia (ELBO)**. La funci贸n ELBO es diferenciable y, por lo tanto, puede optimizarse mediante **Descenso de Gradiente**.



### B. Ventajas y Desventajas de VI

| Aspecto | Ventajas | Desventajas |
| :--- | :--- | :--- |
| **Velocidad y Escalabilidad** | Convierte la inferencia en un problema de optimizaci贸n, lo que permite el uso de SGD. Es **mucho m谩s r谩pido** que MCMC y puede escalar a modelos de *Deep Learning* con millones de par谩metros. | **Sesgo:** La aproximaci贸n $q$ introduce un sesgo. VI no converge a la distribuci贸n verdadera, sino a la mejor aproximaci贸n dentro de la clase de $q$. |
| **Facilidad de Uso** | Compatible con *frameworks* de *Deep Learning* y diferenciaci贸n autom谩tica (Programaci贸n Diferenciable). | **Inflexibilidad:** La elecci贸n de la familia de distribuciones $q$ (ej. independencia entre par谩metros) puede ser una simplificaci贸n demasiado fuerte. |
| **Convergencia** | La convergencia se diagnostica f谩cilmente mediante el monitoreo del ELBO. | Tiende a subestimar la varianza de la posterior verdadera (por la naturaleza de la KL). |

---

## 3. Elecci贸n y Tendencias Modernas

La elecci贸n entre MCMC y VI depende del contexto:

* **MCMC es preferido** cuando la precisi贸n absoluta en la aproximaci贸n de la distribuci贸n es primordial (ej. investigaci贸n te贸rica, problemas de baja dimensi贸n).
* **VI es el est谩ndar de facto** en el **Aprendizaje Profundo Bayesiano (BDL)** debido a sus requisitos de escalabilidad y velocidad.

Las tendencias modernas, como el **Hamiltonian Monte Carlo (HMC)** (una versi贸n de MCMC que utiliza gradientes) y el **MCMC No Convergente** (MCMC ejecutado por un n煤mero limitado de pasos para mejorar la calidad de las muestras de VI), buscan fusionar las ventajas de ambos, logrando un equilibrio entre precisi贸n y velocidad.


---

Continua: [[11-2-1]()] 
