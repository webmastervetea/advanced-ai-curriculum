

## üèóÔ∏è B√∫squeda de Arquitectura Neuronal (NAS): Automatizando el Dise√±o de Modelos

El objetivo del NAS es encontrar la arquitectura $\mathcal{A}$ (definida por el n√∫mero de capas, el tipo de conexiones, las funciones de activaci√≥n y los par√°metros de *skip-connection*) que minimice la p√©rdida $L$ en un *dataset* dado.

El desaf√≠o principal es el **Espacio de B√∫squeda** (el n√∫mero de arquitecturas posibles), que es combinatoriamente inmenso. Las estrategias algor√≠tmicas de NAS se centran en c√≥mo navegar este espacio de manera eficiente.

---

### 1. NAS basado en Aprendizaje por Refuerzo (RL-NAS)

El enfoque RL-NAS formula el problema de dise√±ar una red neuronal como una secuencia de decisiones tomadas por un agente.

#### Mecanismo

1.  **Agente (Controlador):** Es una Red Neuronal Recurrente (RNN) o un Transformador simple entrenado para generar la secuencia de *tokens* que definen la arquitectura (ej., "convoluci√≥n 3x3", "Max-Pooling", "skip-connection").
2.  **Entorno:** El sistema NAS.
3.  **Acciones y Estado:** Cada decisi√≥n del Agente (ej., elegir el tama√±o del kernel) es una acci√≥n que modifica el estado (la arquitectura parcial).
4.  **Recompensa (*Reward*):** Una vez que el Agente genera una arquitectura completa, esta se entrena desde cero y se eval√∫a en el conjunto de validaci√≥n. La precisi√≥n de validaci√≥n es la se√±al de recompensa, que se devuelve al Agente (Controlador).
5.  **Optimizaci√≥n:** El Agente se entrena utilizando algoritmos de pol√≠tica de gradiente (como REINFORCE) para maximizar la recompensa futura (es decir, aprender a generar arquitecturas de alto rendimiento).

#### Ventajas y Limitaciones
| Aspecto | RL-NAS |
| :--- | :--- |
| **Flexibilidad** | Alta. Puede explorar arquitecturas complejas y no est√°ndar. |
| **Eficiencia** | Baja. Requiere entrenar miles de arquitecturas candidatas desde cero, lo que lo hace muy costoso. |

---

### 2. NAS basado en Algoritmos Gen√©ticos (AG-NAS)

Los Algoritmos Gen√©ticos (AG) abordan el NAS imitando el proceso de selecci√≥n natural y la evoluci√≥n.

#### Mecanismo

1.  **Poblaci√≥n Inicial:** Se genera un conjunto de arquitecturas aleatorias ("individuos").
2.  **Fitness (Aptitud):** Cada individuo es entrenado y su rendimiento (precisi√≥n) se eval√∫a, determinando su *fitness*.
3.  **Selecci√≥n:** Se seleccionan las arquitecturas con el *fitness* m√°s alto ("los m√°s aptos para sobrevivir").
4.  **Crossover (Cruce):** Se combinan sub-gr√°ficos o partes de dos arquitecturas de alto rendimiento para crear un nuevo "descendiente" (ej., tomando las primeras capas de la red A y las √∫ltimas capas de la red B).
5.  **Mutaci√≥n:** Se introduce aleatoriedad modificando un solo componente de la arquitectura (ej., cambiar un kernel 5x5 por un kernel 3x3 o a√±adir una conexi√≥n *skip*).
6.  **Iteraci√≥n:** El proceso se repite, generando una poblaci√≥n que evoluciona hacia arquitecturas √≥ptimas.

#### Ventajas y Limitaciones
| Aspecto | AG-NAS |
| :--- | :--- |
| **Exploraci√≥n** | Muy bueno para la exploraci√≥n global y la optimizaci√≥n en paralelo. |
| **Eficiencia** | Media-Baja. Tambi√©n requiere numerosas evaluaciones, aunque las mutaciones suelen ser m√°s efectivas que la b√∫squeda aleatoria de RL. |

---

### 3. NAS basado en Optimizaci√≥n Bayesiana (OB-NAS)

La Optimizaci√≥n Bayesiana (OB) est√° dise√±ada para optimizar una funci√≥n "caja negra" (como la precisi√≥n de validaci√≥n) que es costosa de evaluar. Su objetivo es reducir el n√∫mero de arquitecturas que deben entrenarse por completo.

#### Mecanismo

1.  **Modelo Sustituto (*Surrogate Model*):** Un modelo estad√≠stico, generalmente un Proceso Gaussiano (GP), se entrena sobre los resultados hist√≥ricos. Predice la precisi√≥n de las arquitecturas que **a√∫n no se han entrenado**.
2.  **Funci√≥n de Adquisici√≥n (*Acquisition Function*):** Esta funci√≥n utiliza las predicciones del modelo sustituto para decidir **qu√© arquitectura evaluar a continuaci√≥n**. Equilibra:
    * **Explotaci√≥n:** Elegir una arquitectura que se predice con alta precisi√≥n.
    * **Exploraci√≥n:** Elegir una arquitectura que tiene una gran incertidumbre en su predicci√≥n.
3.  **Iteraci√≥n:** Se eval√∫a la arquitectura seleccionada, se a√±ade el resultado al historial, y se actualiza el modelo sustituto.

#### Ventajas y Limitaciones
| Aspecto | OB-NAS |
| :--- | :--- |
| **Eficiencia de Muestreo**| Alta. Necesita muchas menos evaluaciones que RL o AG. |
| **Escalabilidad** | Baja. El modelo sustituto (GP) escala mal con el n√∫mero de par√°metros de la arquitectura, volvi√©ndose ineficiente en espacios de b√∫squeda de muy alta dimensi√≥n. |

---

### 4. Estrategias Avanzadas para Acelerar la NAS (Mitigaci√≥n del Costo)

El mayor desaf√≠o de la NAS es el costo de entrenar miles de modelos. Las siguientes t√©cnicas son a menudo usadas junto con RL, AG o OB para reducir dr√°sticamente el tiempo de b√∫squeda:

| T√©cnica | Principio | Estrategia de Ahorro |
| :--- | :--- | :--- |
| **Compartici√≥n de Pesos (ENAS)**| Se crea una **SuperRed** que contiene todas las arquitecturas posibles como subgrafos. | Todos los subgrafos comparten los pesos. Solo se entrena la SuperRed una vez, lo que reduce el costo de evaluaci√≥n de un candidato. |
| **Diferenciaci√≥n Continua (DARTS)**| El espacio de b√∫squeda se transforma en un espacio continuo donde la arquitectura √≥ptima puede encontrarse mediante **Descenso de Gradiente**. | La b√∫squeda de arquitectura y el entrenamiento de pesos se realizan simult√°neamente y con el mismo gradiente, lo que ofrece una aceleraci√≥n masiva. |
| **Predicci√≥n de Rendimiento**| En lugar de entrenar completamente una arquitectura, un modelo de *Machine Learning* predice su rendimiento bas√°ndose en m√©tricas tempranas (ej., rendimiento despu√©s de 5 √©pocas). | Permite descartar r√°pidamente las arquitecturas de bajo rendimiento sin incurrir en el costo total del entrenamiento. |

La combinaci√≥n de una estrategia de b√∫squeda eficiente (como RL o OB) con t√©cnicas de aceleraci√≥n como la compartici√≥n de pesos ha transformado el NAS de una curiosidad acad√©mica a una herramienta pr√°ctica utilizada para el dise√±o de arquitecturas en la industria.


---

Continua: [[31-1-2](https://github.com/webmastervetea/advanced-ai-curriculum/blob/main/temarios/31-1-2.md)] 
