

## 游 Optimizaci칩n de Par치metros: LoRA y QLoRA para la Adaptaci칩n Eficiente de LLMs

Los Modelos de Lenguaje Grandes (LLMs) contienen miles de millones de par치metros, lo que hace que su **ajuste fino (*finetuning*)** sea prohibitivamente costoso en t칠rminos de memoria de GPU, tiempo de entrenamiento y almacenamiento. Las t칠cnicas de **Ajuste Fino de Par치metros Eficientes (PEFT, *Parameter-Efficient Finetuning*)** buscan mitigar este problema al actualizar solo un peque침o subconjunto de los par치metros del modelo, siendo **LoRA** la metodolog칤a m치s destacada.

---

### 1. El Problema del Finetuning Completo

Al afinar un LLM tradicionalmente, se actualizan todas las capas de la red neuronal. Esto requiere:

1.  **Alto Almacenamiento:** Guardar una copia completa de los pesos del modelo ajustados.
2.  **Alta Memoria:** Mantener en la VRAM de la GPU no solo los pesos del modelo original, sino tambi칠n los gradientes y el optimizador para todos los miles de millones de par치metros.
3.  **Lento Entrenamiento:** Recalcular gradientes y actualizar la memoria para cada par치metro.

---

### 2. LoRA: Low-Rank Adaptation (Adaptaci칩n de Rango Bajo)

LoRA se basa en la hip칩tesis de que las actualizaciones de peso que ocurren durante el *finetuning* tienen una **clasificaci칩n intr칤nsecamente baja** (*low-rank*), lo que significa que pueden ser representadas de manera eficiente por matrices mucho m치s peque침as.

#### A. La Matriz de Actualizaci칩n ($\Delta W$)

Consideremos una matriz de pesos pre-entrenada ($W_0$) dentro de una capa del LLM (t칤picamente una matriz lineal en las capas de Atenci칩n o *Feed-Forward*). El *finetuning* completo aprende una matriz de actualizaci칩n ($\Delta W$) tal que:

$$W_{nuevo} = W_0 + \Delta W$$

En LoRA, en lugar de aprender y almacenar $\Delta W$ (que tiene el mismo tama침o que $W_0$, p. ej., $d \times k$), se factoriza $\Delta W$ en el producto de dos matrices de rango bajo mucho m치s peque침as, $A$ y $B$:

$$\Delta W \approx B A$$

* **Matriz B:** De dimensi칩n $d \times r$.
* **Matriz A:** De dimensi칩n $r \times k$.
* **Rango Bajo ($r$):** El hiperpar치metro $r$ (p. ej., $r=4$ o $r=8$) es mucho menor que las dimensiones originales $d$ y $k$ (p. ej., $d=4096$).



#### B. Beneficios Clave de LoRA

1.  **Par치metros Entrenables Reducidos:** Solo se entrenan los pesos de las matrices $A$ y $B$. El n칰mero de par치metros entrenables puede reducirse en un **factor de 1,000 a 10,000**.
2.  **Inferencia R치pida:** Durante la inferencia, se puede calcular $B A$ y sumarlo a $W_0$ para obtener la matriz de pesos final. O, de manera m치s eficiente, se pueden sumar las salidas de $W_0 x$ y $(B A) x$.
3.  **Compartici칩n y Despliegue:** Se pueden entrenar m칰ltiples adaptadores $(A_i, B_i)$ para diferentes tareas y acoplar ("plug and play") el adaptador deseado al modelo base $W_0$ en tiempo de ejecuci칩n.

---

### 3. QLoRA: Quantization-aware Low-Rank Adaptation

**QLoRA** es la evoluci칩n de LoRA que aborda el problema de la **memoria de VRAM** a칰n m치s agresivamente mediante la **cuantificaci칩n** de los pesos del modelo base.

#### A. El Problema de la Memoria

Aunque LoRA reduce los par치metros entrenables, el modelo base $W_0$ (miles de millones de par치metros) todav칤a necesita cargarse en la memoria de la GPU, consumiendo grandes cantidades de VRAM.

#### B. Mecanismo de QLoRA

QLoRA hace tres contribuciones principales:

1.  **Cuantificaci칩n de 4 bits (4-bit Normal Float, NF4):** El modelo base $W_0$ se cuantifica de 16 bits a solo **4 bits por par치metro**. Esto reduce el consumo de memoria del modelo base en 4x.
2.  **Cuantificaci칩n de Doble Cuantificaci칩n (*Double Quantization*):** Reduce a칰n m치s el tama침o de almacenamiento al cuantificar las constantes de cuantificaci칩n (los par치metros utilizados para des-cuantificar) ellas mismas.
3.  **Memoria Paginada (*Paged Optimizers*):** Utiliza t칠cnicas de gesti칩n de memoria similares a la memoria virtual de la CPU para evitar el error de "memoria agotada" al mover temporalmente las matrices de gradiente y optimizaci칩n entre la VRAM de la GPU y la RAM de la CPU.

#### C. Proceso de Entrenamiento en QLoRA

1.  El modelo base $W_0$ se carga en VRAM cuantificado a 4 bits (lectura, no modificable).
2.  Se adjuntan los adaptadores LoRA $(A, B)$ al modelo base.
3.  Durante el *finetuning*, el modelo des-cuantifica din치micamente solo los pesos necesarios para el c치lculo (*forward/backward pass*) y solo actualiza los par치metros de $A$ y $B$.
4.  La matriz $W_0$ de 4 bits nunca se toca, por lo que el uso de memoria es m칤nimo.

---

### 4. Conclusi칩n

LoRA y QLoRA han democratizado el *finetuning* de LLMs. LoRA ofrece una **reducci칩n masiva de par치metros entrenables y almacenamiento de *checkpoints***, mientras que QLoRA lo combina con la **cuantificaci칩n** para permitir que modelos gigantes como Llama 2 (70B par치metros) se afinen en GPU de consumo (p. ej., una 칰nica NVIDIA RTX 3090/4090), abriendo nuevas posibilidades para la investigaci칩n, la personalizaci칩n y el despliegue de IA a peque침a escala.

---

Continua: [[24.3.1](https://github.com/webmastervetea/advanced-ai-curriculum/blob/main/temarios/24-3-1.md)] 
