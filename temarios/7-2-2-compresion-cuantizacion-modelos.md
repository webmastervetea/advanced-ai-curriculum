#  Compresi贸n y Cuantizaci贸n de Modelos para Edge AI

El objetivo de la **Inteligencia Artificial en el Borde (*Edge AI*)** es ejecutar modelos de *Deep Learning* directamente en dispositivos locales (m贸viles, IoT, *wearables*) en lugar de en la nube. Esto garantiza **baja latencia**, **privacidad** y **fiabilidad** (independencia de la red). Para lograrlo, los modelos deben ser dr谩sticamente reducidos en tama帽o y complejidad.

## 1. Cuantizaci贸n (*Quantization*): Reducci贸n de Precisi贸n Num茅rica

La **Cuantizaci贸n** es el proceso de reducir la precisi贸n num茅rica de los pesos y activaciones del modelo. Esto tiene el impacto m谩s significativo e inmediato en la reducci贸n del tama帽o del modelo y en el consumo de energ铆a.

### A. Mecanismo Fundamental

Los modelos de *Deep Learning* se entrenan t铆picamente utilizando n煤meros de punto flotante de 32 bits (**FP32**). La cuantizaci贸n mapea estos valores a representaciones de menor precisi贸n, generalmente n煤meros enteros de 8 bits (**INT8**).

* **Reducci贸n de Memoria:** Pasar de FP32 a INT8 reduce el tama帽o de almacenamiento del modelo y la memoria en tiempo de ejecuci贸n en **cuatro veces**.
* **Aceleraci贸n:** Los procesadores especializados de *Edge AI* (como los NPUs o los aceleradores m贸viles) est谩n optimizados para realizar c谩lculos con INT8 mucho m谩s r谩pido y con menos energ铆a que con FP32.

### B. Tipos de Cuantizaci贸n

1.  **Post-Training Quantization (PTQ):** La cuantizaci贸n se realiza **despu茅s** de que el modelo ha sido completamente entrenado. Es el m茅todo m谩s r谩pido y simple. Solo requiere un peque帽o conjunto de datos de calibraci贸n para determinar los rangos de mapeo de los valores FP32 a INT8.
    * **Limitaci贸n:** Puede causar una ligera p茅rdida de precisi贸n si el rango de valores de los pesos es muy amplio o desigual.
2.  **Quantization-Aware Training (QAT):** El modelo se entrena o se somete a un ajuste fino (*fine-tuning*) utilizando operaciones simuladas de baja precisi贸n.
    * **Ventaja:** El modelo se vuelve **consciente** de los errores de cuantizaci贸n y ajusta sus pesos para compensarlos. Esto resulta en una precisi贸n mucho mayor, a menudo igualando o superando la precisi贸n del modelo base FP32.
    * **Desventaja:** Requiere el proceso de entrenamiento o ajuste fino y el conjunto de datos de entrenamiento completo.

---

## 2. Poda de Modelos (*Model Pruning*): Reducci贸n de la Redundancia

La **Poda** es el proceso de eliminar las conexiones o neuronas del modelo que tienen poco impacto en la precisi贸n, bas谩ndose en la idea de que la mayor铆a de los modelos de *Deep Learning* est谩n **sobredimensionados** y contienen redundancia.

### A. Mecanismo de Poda

1.  **Identificaci贸n:** Se eval煤a la **importancia** de los pesos de la red (t铆picamente midiendo su magnitud absoluta). Los pesos cercanos a cero se consideran poco importantes.
2.  **Eliminaci贸n (*Clipping*):** Los pesos no importantes se establecen a cero.
3.  **Ajuste Fino:** Se realiza un ajuste fino del modelo (con los pesos restantes) para recuperar cualquier peque帽a p茅rdida de precisi贸n causada por la poda.

### B. Tipos de Poda

* **Poda No Estructurada (Sparcity):** Se eliminan individualmente los pesos menos importantes en cualquier lugar de las matrices.
    * **Ventaja:** Logra la mayor tasa de compresi贸n (ej. hasta el 90-95% de los pesos).
    * **Desventaja:** El patr贸n disperso resultante es dif铆cil de acelerar con el *hardware* de prop贸sito general (GPUs/CPUs), aunque es 煤til en *hardware* especializado para matrices dispersas.
* **Poda Estructurada:** Se eliminan bloques enteros de la red (ej. filas o columnas de una matriz de pesos, o canales completos en una red convolucional).
    * **Ventaja:** La estructura regular resultante es **amigable con el *hardware*** est谩ndar (GPUs) y facilita el uso de bibliotecas optimizadas para lograr una aceleraci贸n real en el tiempo de ejecuci贸n.

---

## 3. Destilaci贸n del Conocimiento (*Knowledge Distillation*): El Estudiante y el Maestro

La **Destilaci贸n del Conocimiento** es una t茅cnica de compresi贸n basada en el entrenamiento donde se utiliza un modelo grande y de alto rendimiento (**Modelo Maestro**) para entrenar un modelo mucho m谩s peque帽o y r谩pido (**Modelo Estudiante**).

### A. Mecanismo

1.  **Modelo Maestro (*Teacher*):** El modelo grande y lento (ej. un LLM completo) se entrena primero para lograr la m谩xima precisi贸n. Sus salidas (t铆picamente los ***logits* o las probabilidades suaves**) contienen el conocimiento "destilado" del entrenamiento.
2.  **Modelo Estudiante (*Student*):** El modelo peque帽o y eficiente (ej. un Transformer con menos capas o menos cabezas de atenci贸n) se entrena para imitar las salidas del Maestro.
3.  **Funci贸n de P茅rdida Doble:** El Estudiante se entrena con una funci贸n de p茅rdida que incluye dos componentes:
    * **P茅rdida Hard Target:** La p茅rdida est谩ndar calculada a partir de las etiquetas reales.
    * **P茅rdida Soft Target:** La p茅rdida calculada a partir de la diferencia entre la salida del Estudiante y la salida "suave" (con alta temperatura) del Maestro.



### B. Ventajas

* **Rendimiento Superior:** El Estudiante, aunque tiene menos par谩metros, a menudo logra una precisi贸n significativamente **mayor** que la de un modelo entrenado de forma independiente con la misma arquitectura peque帽a. Esto se debe a que el Maestro act煤a como un fuerte regularizador, proporcionando etiquetas m谩s informativas y menos ruidosas.
* **Flexibilidad:** El Estudiante puede tener una arquitectura completamente diferente al Maestro (ej. Destilar un Transformer grande en un modelo RNN o CNN peque帽o).

---

## 4. Combinaci贸n de T茅cnicas para el Despliegue en Edge

En la pr谩ctica, estas t茅cnicas se utilizan a menudo en secuencia para lograr la m谩xima compresi贸n y aceleraci贸n:

1.  **Destilaci贸n:** Se reduce la complejidad arquitect贸nica (de Maestro a Estudiante peque帽o).
2.  **Poda:** Se elimina la redundancia de los pesos del Modelo Estudiante.
3.  **Cuantizaci贸n:** Se convierten los pesos y activaciones resultantes a INT8.

Al pasar de un modelo FP32 sobredimensionado a un modelo INT8 podado y destilado, se puede lograr una reducci贸n de la memoria de **$20\times$ a $50\times$** y una aceleraci贸n de **$5\times$ a $10\times$** en la inferencia, lo que hace posible el *Edge AI* de alto rendimiento.


---

Continua: [[7-3-1](https://github.com/webmastervetea/advanced-ai-curriculum/blob/main/temarios/7-3-1-deteccion-deriva-de-datos.md)] 
