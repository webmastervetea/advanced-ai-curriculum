# üõ°Ô∏è Mitigaci√≥n de Sesgos en Datos y Modelos: Hacia una IA Equitativa

El **sesgo** en el contexto de la IA se refiere a errores sistem√°ticos y a menudo injustos en el proceso de toma de decisiones de un modelo, generalmente influenciados por desequilibrios o prejuicios presentes en los datos de entrenamiento. Estos sesgos pueden llevar a un rendimiento desigual para diferentes **grupos demogr√°ficos protegidos** (como raza, g√©nero, edad, o ingresos).

## 1. Tipos Comunes de Sesgos en Machine Learning

Es esencial distinguir entre los sesgos que afectan a la recopilaci√≥n de datos y los que afectan al modelado:

### A. Sesgos en los Datos (Data Bias)

* **Sesgo de Muestreo (*Sampling Bias*):** Ocurre cuando el conjunto de datos utilizado para entrenar el modelo no es representativo de la poblaci√≥n real.
    * *Ejemplo:* Un sistema de reconocimiento facial entrenado principalmente con im√°genes de personas cauc√°sicas tendr√° un rendimiento significativamente peor en individuos de otras etnias.
* **Sesgo de Etiquetado (*Label Bias*):** Ocurre cuando las etiquetas utilizadas para el aprendizaje supervisado reflejan prejuicios humanos o creencias hist√≥ricamente injustas.
    * *Ejemplo:* Etiquetar ciertas interacciones como "agresivas" en un conjunto de datos, donde la definici√≥n de "agresivo" puede estar sesgada culturalmente o basada en estereotipos raciales/de g√©nero.
* **Sesgo Hist√≥rico (*Historical Bias*):** El sesgo es inherente a la sociedad y est√° codificado en los datos hist√≥ricos. Incluso si los datos son precisos, reflejan injusticias pasadas.
    * *Ejemplo:* Entrenar un modelo de contrataci√≥n con datos hist√≥ricos donde los hombres ocupaban la mayor√≠a de los puestos de liderazgo sesgar√° el modelo para favorecer a los candidatos masculinos, sin importar las cualificaciones.

### B. Sesgos en el Modelado (Algorithm/Model Bias)

* **Sesgo de Agregaci√≥n (*Aggregation Bias*):** Ocurre cuando se aplica un modelo √∫nico a un grupo muy heterog√©neo, ignorando las diferencias importantes entre subgrupos.
    * *Ejemplo:* Usar un √∫nico modelo predictivo de riesgo de enfermedad para toda la poblaci√≥n, cuando la enfermedad se manifiesta de forma diferente en hombres y mujeres, o en distintos grupos de edad.
* **Sesgo de Medida (*Measurement Bias*):** El *proxy* (variable sustituta) utilizado para medir un concepto resulta ser un predictor sesgado.
    * *Ejemplo:* Usar el historial de arrestos como *proxy* de "criminalidad" futura, cuando las tasas de arresto pueden reflejar el sesgo policial en lugar de la actividad delictiva real.

---

## 2. Estrategias de Mitigaci√≥n en el Ciclo de Vida del ML

La mitigaci√≥n debe abordarse en tres fases clave para ser efectiva:

### Fase I: Pre-Procesamiento (Mitigaci√≥n en los Datos)

El objetivo es eliminar o reducir el sesgo de los datos antes de que lleguen al modelo.

1.  **Detecci√≥n de Sesgo y Exploraci√≥n (EDA):** Analizar la distribuci√≥n de los datos para identificar desequilibrios en grupos protegidos. 
    * *T√©cnica:* Calcular las **Tasas de Paridad Demogr√°fica** (la proporci√≥n de la m√©trica de inter√©s entre grupos).
2.  **Re-muestreo (*Resampling*):** Ajustar el n√∫mero de instancias en los subgrupos minoritarios para asegurar la paridad en el conjunto de entrenamiento.
3.  **Supresi√≥n de Sesgo (*Suppressing Bias*):** Eliminar o modificar expl√≠citamente atributos sesgados o altamente correlacionados con el sesgo (por ejemplo, remover c√≥digo postal si se demuestra que induce sesgo socioecon√≥mico). *Advertencia:* Esto puede ser insuficiente, ya que el sesgo puede estar codificado en otros atributos.
4.  **Aprendizaje de Representaciones Invariantes (Adversarial Debiasing):** Entrenar un modelo auxiliar para asegurar que la representaci√≥n de los datos sea indistinguible entre diferentes grupos protegidos, forzando al modelo principal a ignorar esa informaci√≥n.

---

### Fase II: Procesamiento (Mitigaci√≥n en el Modelo)

El objetivo es modificar el algoritmo de entrenamiento para incorporar la justicia como un factor de optimizaci√≥n.

1.  **Restricciones de Equidad (*Fairness Constraints*):** Modificar la funci√≥n de p√©rdida del modelo. En lugar de solo minimizar el error predictivo, la nueva funci√≥n de p√©rdida incluye un t√©rmino de penalizaci√≥n si se violan ciertas m√©tricas de equidad.
    $$\mathcal{L}_{\text{final}} = \mathcal{L}_{\text{predicci√≥n}} + \lambda \cdot \mathcal{L}_{\text{equidad}}$$
    Donde $\lambda$ es un hiperpar√°metro que equilibra la precisi√≥n y la equidad.
2.  **Aprendizaje Adversario (*Adversarial Learning*):** Utilizar una arquitectura similar a las GANs. Se entrena un **predictor de justicia** (un discriminador) para predecir a qu√© grupo protegido pertenece un individuo bas√°ndose en las representaciones latentes del modelo principal. El modelo principal se entrena para **enga√±ar** a este predictor, creando representaciones que no contengan informaci√≥n de sesgo, sin sacrificar la precisi√≥n predictiva.

---

### Fase III: Post-Procesamiento (Mitigaci√≥n en las Predicciones)

El objetivo es ajustar el resultado final del modelo para garantizar la equidad antes de que se implemente la decisi√≥n.

1.  **Clasificaci√≥n Equitativa (*Equalized Odds*):** Ajustar los umbrales de decisi√≥n del modelo (el punto de corte de probabilidad para clasificar como positivo) de manera diferente para cada grupo protegido.
    * *Objetivo:* Lograr la paridad en las **tasas de falsos positivos** y **tasas de falsos negativos** entre los grupos. Por ejemplo, si el modelo tiene una tasa de falsos positivos del 5% en el Grupo A y del 15% en el Grupo B, se ajustan los umbrales para igualar estas tasas.
2.  **Paridad de Oportunidad (*Equal Opportunity*):** Una m√©trica menos estricta que solo requiere igualar la **tasa de verdaderos positivos** (recall) entre los grupos para una clasificaci√≥n espec√≠fica. Esto se aplica a escenarios donde el resultado positivo es un beneficio (ej. pr√©stamos, admisi√≥n universitaria).
3.  **Calibraci√≥n:** Asegurar que las probabilidades de predicci√≥n del modelo reflejen con precisi√≥n las verdaderas probabilidades para todos los subgrupos.

---

## 3. Desaf√≠os y Consideraciones √âticas

* **Trade-off entre Equidad y Precisi√≥n:** A menudo, los m√©todos de mitigaci√≥n de sesgos requieren sacrificar una peque√±a cantidad de precisi√≥n general del modelo para lograr una mayor equidad. La decisi√≥n sobre d√≥nde trazar la l√≠nea es √©tica y de pol√≠tica, no solo t√©cnica.
* **Imposibilidad de la Equidad Total:** No es matem√°ticamente posible satisfacer todas las definiciones de equidad simult√°neamente (por ejemplo, es imposible garantizar la paridad demogr√°fica y las tasas de falsos positivos iguales al mismo tiempo). Se debe elegir la m√©trica de equidad m√°s relevante para el dominio de la aplicaci√≥n (ej. en sistemas penales, la tasa de falsos positivos es cr√≠tica).
* **Interseccionalidad:** Los m√©todos de mitigaci√≥n a menudo se centran en un √∫nico atributo (g√©nero o raza), ignorando los sesgos experimentados por subgrupos minoritarios m√∫ltiples (mujeres negras de bajos ingresos).

La mitigaci√≥n de sesgos es un proceso continuo que requiere un esfuerzo multidisciplinario y una supervisi√≥n constante para garantizar que los sistemas de IA sigan siendo justos y √©ticos despu√©s de la implementaci√≥n.


---

Continua: [[6-2]()] 
