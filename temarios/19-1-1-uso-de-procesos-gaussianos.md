#  Procesos Gaussianos: Modelado de Funciones Caras y Ruidosas

Los **Procesos Gaussianos (*Gaussian Processes*, GPs)** son una herramienta estad铆stica no param茅trica poderosa utilizada para modelar distribuciones sobre funciones. A diferencia de los modelos param茅tricos que asumen una forma funcional fija (ej. lineal o polinomial) con un n煤mero finito de par谩metros, un GP define una **distribuci贸n de probabilidad sobre el espacio de todas las posibles funciones**.

Esta capacidad 煤nica los hace indispensables en escenarios donde la funci贸n objetivo que se intenta optimizar o modelar es costosa de evaluar (por ejemplo, requiere experimentos f铆sicos, simulaciones complejas, o el entrenamiento de un modelo de *Deep Learning*), o cuando las observaciones est谩n contaminadas por ruido.

## 1. Fundamentos: Una Distribuci贸n sobre Funciones

Un Proceso Gaussiano se puede entender como una **extensi贸n multivariada de la distribuci贸n Normal (Gaussiana)**, pero cuyas variables son los valores de la funci贸n en diferentes puntos.

### A. La Definici贸n del GP

Un GP est谩 completamente especificado por su **funci贸n de media ($\mu$)** y su **funci贸n de covarianza ($\mathbf{K}$)**.

1.  **Funci贸n de Media ($\mu(\mathbf{x})$):** Define el valor esperado de la funci贸n en cualquier punto $\mathbf{x}$. A menudo se asume que es cero ($\mu(\mathbf{x}) = 0$) por simplicidad.
2.  **Funci贸n de Covarianza (*Kernel*, $\mathbf{K}(\mathbf{x}_i, \mathbf{x}_j)$):** Es el componente cr铆tico. Define la **similitud** entre dos puntos $\mathbf{x}_i$ y $\mathbf{x}_j$ en el espacio de entrada, y por lo tanto, la correlaci贸n entre los valores de la funci贸n en esos puntos.

> **Principio Clave:** Los puntos de entrada que son similares deber铆an tener valores de salida de la funci贸n similares (suaves) y, por lo tanto, altamente correlacionados.

El *kernel* m谩s com煤n es el **RBF (*Radial Basis Function*)** o *Squared Exponential*, que asume que la correlaci贸n disminuye exponencialmente con la distancia euclidiana entre los puntos.

### B. Inferencia: Creaci贸n del Modelo Predictivo

Dado un conjunto de datos observados ($\mathcal{D} = \{(\mathbf{x}_i, y_i)\}$), el GP calcula la distribuci贸n posterior de las funciones.

* Para cualquier nuevo punto $\mathbf{x}_{\star}$, el GP produce una **distribuci贸n Gaussiana** completa de posibles resultados.
* Esta distribuci贸n se caracteriza por una **predicci贸n de media** (la predicci贸n m谩s probable, $\mu(\mathbf{x}_{\star})$) y una **varianza** (la incertidumbre sobre esa predicci贸n, $\sigma^2(\mathbf{x}_{\star})$).



---

## 2. Aplicaci贸n Central: Optimizaci贸n Bayesiana (BO)

El uso principal de los Procesos Gaussianos es como **modelo sustituto (*surrogate model*)** en la **Optimizaci贸n Bayesiana (BO)**. La BO es una estrategia secuencial para optimizar funciones objetivo caras ($f(\mathbf{x})$).

El ciclo de la BO opera en dos pasos iterativos:

### A. 1. Modelo Sustituto (GP)

El GP se ajusta a los datos de las evaluaciones anteriores. Esto proporciona una **predicci贸n de la media** y, crucialmente, una **estimaci贸n de la incertidumbre (varianza)** para todos los puntos del espacio de b煤squeda.

### B. 2. Funci贸n de Adquisici贸n (*Acquisition Function*)

La funci贸n de adquisici贸n utiliza la media y la varianza del GP para decidir **d贸nde evaluar la funci贸n objetivo a continuaci贸n** (es decir, qu茅 $\mathbf{x}$ probar).

La funci贸n de adquisici贸n equilibra la **Exploraci贸n** (ir a regiones con alta incertidumbre, $\sigma^2$ alta) y la **Explotaci贸n** (ir a regiones con la mejor media prevista, $\mu$ alta).

* **Ejemplo Com煤n:** La **Mejora Esperada (*Expected Improvement*, EI)**. Esta funci贸n mide la mejora esperada sobre el mejor valor conocido hasta ahora ($y_{\text{best}}$), sopesada por la incertidumbre.
* **Decisi贸n:** Se elige el punto $\mathbf{x}_{\text{next}}$ que maximiza el valor de la funci贸n de adquisici贸n.

Finalmente, la funci贸n objetivo real se eval煤a en $\mathbf{x}_{\text{next}}$, y el nuevo par $(\mathbf{x}_{\text{next}}, y_{\text{next}})$ se a帽ade al conjunto de datos $\mathcal{D}$, cerrando el ciclo.

## 3. Manejo de Funciones Ruidosas

Los Procesos Gaussianos manejan el ruido inherente a los datos de forma natural a trav茅s de su modelo probabil铆stico.

### A. Ruido como Par谩metro

En un GP, el ruido se modela a帽adiendo un t茅rmino de varianza ($\sigma_n^2$) al *kernel* en la diagonal de la matriz de covarianza.

$$K_{\text{ruido}}(\mathbf{x}_i, \mathbf{x}_j) = K(\mathbf{x}_i, \mathbf{x}_j) + \delta_{ij}\sigma_n^2$$

Donde $\delta_{ij}$ es la delta de Kronecker (igual a 1 si $i=j$, y 0 si $i \ne j$).

* **Efecto:** El GP no tiene que pasar exactamente por los puntos de datos ruidosos, lo que permite que la funci贸n de media se mantenga suave y represente la verdadera funci贸n subyacente, mientras que la varianza explica el ruido de la medici贸n.

## 4. Conclusi贸n

Los Procesos Gaussianos son una herramienta esencial para la optimizaci贸n y el modelado de *Machine Learning* en entornos del mundo real. Al proporcionar no solo una predicci贸n puntual, sino tambi茅n una cuantificaci贸n de la incertidumbre, el GP permite a los algoritmos de Optimizaci贸n Bayesiana tomar decisiones racionalmente informadas sobre el equilibrio entre exploraci贸n y explotaci贸n, lo que resulta en una convergencia eficiente y una reducci贸n dr谩stica del costo de las evaluaciones.


---

Continua: [[19-1-2](https://github.com/webmastervetea/advanced-ai-curriculum/blob/main/temarios/19-1-2-aplicacion-de-bo.md)] 
