# 游꺕 Algoritmos de B칰squeda Avanzados: Monte Carlo Tree Search (MCTS)

El **Monte Carlo Tree Search (MCTS)** es un algoritmo de b칰squeda heur칤stica probabil칤stica que se utiliza para la toma de decisiones en dominios complejos, especialmente en juegos de informaci칩n perfecta (como el ajedrez o el Go). Se hizo mundialmente famoso como el algoritmo principal utilizado por **AlphaGo** de DeepMind, que derrot칩 a los campeones mundiales de Go.

MCTS combina la exploraci칩n aleatoria de los m칠todos de Monte Carlo con la b칰squeda sistem치tica de un 치rbol de juego, equilibrando la exploraci칩n de nuevos movimientos con la explotaci칩n de los movimientos prometedores ya encontrados.

## 1. El Problema que Resuelve MCTS

Los algoritmos de b칰squeda tradicionales (como el *Minimax* o *Alpha-Beta Pruning*) funcionan bien en juegos con espacios de estado limitados (como el ajedrez), pero fallan en juegos con un **factor de ramificaci칩n** extremadamente alto, como el Go (donde el n칰mero de movimientos posibles en cada turno es enorme).

MCTS resuelve esto al centrar sus recursos computacionales en la **parte m치s prometedora** del 치rbol de b칰squeda, sin requerir una funci칩n de evaluaci칩n est치tica para cada estado.

## 2. Las Cuatro Fases del MCTS

El algoritmo MCTS realiza iteraciones continuas. En cada iteraci칩n, un 치rbol de b칰squeda se construye o se expande a trav칠s de cuatro pasos secuenciales:

### A. 1. Selecci칩n (*Selection*)

El proceso comienza en el nodo ra칤z (el estado actual del juego). El algoritmo recorre el 치rbol de b칰squeda existente, seleccionando el nodo hijo m치s prometedor en cada nivel hasta llegar a un nodo **hoja** (un nodo que a칰n no ha sido explorado completamente).

* **Exploraci칩n vs. Explotaci칩n:** La selecci칩n se gu칤a por la m칠trica **Upper Confidence Bound 1 applied to Trees (UCT)**. UCT equilibra el **Exploitation** (elegir el movimiento que hist칩ricamente ha dado el mejor resultado) con la **Exploration** (elegir movimientos que han sido explorados muy pocas veces).
$$UCT = \frac{v_i}{n_i} + c \sqrt{\frac{\ln N}{n_i}}$$
Donde:
* $\frac{v_i}{n_i}$ es el t칠rmino de **explotaci칩n** (ratio de victorias del nodo $i$).
* $c \sqrt{\frac{\ln N}{n_i}}$ es el t칠rmino de **exploraci칩n** (desalienta la sobre-exploraci칩n de nodos visitados).


### B. 2. Expansi칩n (*Expansion*)

Una vez que la selecci칩n llega a un nodo hoja no completamente explorado, se crea **un (o m치s)** nodo hijo nuevo para el nodo hoja. Este nodo reci칠n creado representa un movimiento legal nunca antes simulado.

### C. 3. Simulaci칩n (*Simulation/Rollout*)

Desde el nodo reci칠n expandido, se realiza una **simulaci칩n completa del juego** hasta que se alcanza un estado terminal (el juego termina).

* **Estrategia:** Esta simulaci칩n (*rollout*) se realiza utilizando una **pol칤tica por defecto** (generalmente una pol칤tica puramente aleatoria o una heur칤stica simple y r치pida, ya que el objetivo es solo obtener un resultado r치pido).

### D. 4. Retropropagaci칩n (*Backpropagation*)

El resultado de la simulaci칩n (victoria, derrota o empate) se utiliza para **actualizar las estad칤sticas** de todos los nodos en el camino desde el nodo expandido hasta la ra칤z.

* **Actualizaci칩n:** El contador de **visitas** ($n_i$) de cada nodo y su contador de **victorias** ($v_i$) se actualizan.

Este proceso se repite miles o millones de veces dentro de un tiempo de pensamiento limitado, construyendo un 치rbol de juego asim칠trico y altamente enfocado.

## 3. La Decisi칩n Final

Una vez que se agota el tiempo asignado para la b칰squeda, el algoritmo no elige el movimiento con la tasa de victorias m치s alta, sino el nodo hijo de la ra칤z que ha sido **visitado m치s veces**.

* **Raz칩n:** El n칰mero de visitas refleja el movimiento que el algoritmo ha considerado m치s prometedor y al que ha dedicado la mayor parte de sus recursos de simulaci칩n.

## 4. MCTS y Deep Learning (AlphaGo)

Si bien el MCTS tradicional es poderoso, su rendimiento depende de la calidad de la pol칤tica por defecto utilizada en la simulaci칩n. **AlphaGo** combin칩 MCTS con **Redes Neuronales Profundas (*Deep Learning*)** para mejorar dr치sticamente las fases 1 y 3.

### A. Pol칤tica (*Policy Network*) y Selecci칩n

AlphaGo utiliz칩 una **Red de Pol칤tica (*Policy Network*)** (una DNN) entrenada para imitar a jugadores expertos.

* **Impacto en MCTS:** En lugar de usar la f칩rmula UCT para todos los movimientos, la Red de Pol칤tica suger칤a los movimientos m치s probables y **sesgaba la exploraci칩n** hacia ellos, podando el 99% de las ramas in칰tiles en el Go.

### B. Evaluaci칩n (*Value Network*) y Rollout

AlphaGo reemplaz칩 la simulaci칩n *rollout* simple con una **Red de Valor (*Value Network*)** (otra DNN).

* **Impacto en MCTS:** Cuando MCTS llega a un nodo hoja, en lugar de simular aleatoriamente hasta el final, la Red de Valor **estima la probabilidad de victoria** a partir de ese estado. Esto ahorr칩 mucho tiempo de c칩mputo que de otro modo se gastar칤a en simulaciones, permitiendo que el 치rbol creciera m치s profundamente y de forma m치s precisa.

La combinaci칩n de la eficiencia de b칰squeda de MCTS con la percepci칩n profunda de las redes neuronales cre칩 un sistema de IA que super칩 la capacidad humana en la estrategia de juego.
